# -*- coding: utf-8 -*-
"""vietstock _ crawl (demo)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YEExJlNlMRo60tUlNabwu06GZfA873gx
"""

from google.colab import drive
drive.mount('/content/drive')

print("Google Drive mounted successfully at /content/drive")

# Cell 1: Installations and Imports
# Install necessary packages (run once)
!pip install requests beautifulsoup4 pandas selenium google-generativeai google-auth gspread pytz fuzzywuzzy python-Levenshtein google-auth-oauthlib google-auth-httplib2 -q

# --- Colab Specific Setup for Chrome/ChromeDriver ---
# (This block MUST be run successfully)
print("Updating apt and installing prerequisites...")
!sudo apt-get update >> /dev/null
!sudo apt-get install -y wget gnupg unzip >> /dev/null
print("Downloading and installing Google Chrome...")
!wget -q -O google-chrome-stable_current_amd64.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!sudo dpkg -i google-chrome-stable_current_amd64.deb >> /dev/null
!sudo apt-get install -f -y >> /dev/null # Install dependencies
!rm google-chrome-stable_current_amd64.deb # Clean up
print("Downloading and installing ChromeDriver...")

import subprocess
import os

try:
    chrome_version_full_bytes = subprocess.check_output(['google-chrome', '--version'])
    chrome_version_full = chrome_version_full_bytes.decode('utf-8').strip()
    try:
        chrome_version = chrome_version_full.split(' ')[-1].split('.')[0]
    except IndexError:
        print(f"ERROR: Could not parse Chrome version from: '{chrome_version_full}'")
        raise ValueError("Failed to parse Chrome version")
    print(f"Detected Chrome version: {chrome_version_full} (Major: {chrome_version})")

    wget_command = f'wget -qO- "https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_{chrome_version}"'
    chromedriver_version_bytes = subprocess.check_output(wget_command, shell=True, stderr=subprocess.PIPE)
    chromedriver_version = chromedriver_version_bytes.decode('utf-8').strip()
    if not chromedriver_version or "Error" in chromedriver_version:
         raise ValueError(f"Could not find matching ChromeDriver version for Chrome {chrome_version}. Wget output: {chromedriver_version}")
    print(f"Using ChromeDriver version: {chromedriver_version}")

    print("Downloading ChromeDriver zip...")
    !wget -q -O chromedriver-linux64.zip "https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$chromedriver_version/linux64/chromedriver-linux64.zip"
    print("Unzipping ChromeDriver...")
    !unzip -q chromedriver-linux64.zip -d /usr/local/bin/

    chromedriver_unzipped_path = "/usr/local/bin/chromedriver-linux64/chromedriver"
    chromedriver_direct_path = "/usr/local/bin/chromedriver"

    if os.path.exists(chromedriver_unzipped_path):
        print(f"Moving {chromedriver_unzipped_path} to {chromedriver_direct_path}...")
        !sudo mv {chromedriver_unzipped_path} {chromedriver_direct_path}
        print("Cleaning up extracted directory...")
        !rm -r /usr/local/bin/chromedriver-linux64
    elif os.path.exists(chromedriver_direct_path):
        print("ChromeDriver found directly in /usr/local/bin/.")
    else:
        # Try current directory fallback
        if os.path.exists("./chromedriver-linux64/chromedriver"):
             print("Found chromedriver in current directory structure, attempting move...")
             !sudo mv ./chromedriver-linux64/chromedriver {chromedriver_direct_path}
             !rm -r ./chromedriver-linux64
        elif os.path.exists("./chromedriver"):
             print("Found chromedriver directly in current directory, attempting move...")
             !sudo mv ./chromedriver {chromedriver_direct_path}
        else:
            raise FileNotFoundError("Could not locate chromedriver executable after unzip.")

    print("Setting execution permissions...")
    !sudo chmod +x /usr/local/bin/chromedriver
    print("Cleaning up zip file...")
    !rm chromedriver-linux64.zip

    if os.path.exists(chromedriver_direct_path):
        print(f"ChromeDriver successfully installed to: {chromedriver_direct_path}")
    else:
        raise FileNotFoundError("ChromeDriver final installation check failed.")

except Exception as e:
    print(f"ERROR during automatic ChromeDriver download/setup: {e}")
    print("Falling back to installing a fixed ChromeDriver version via apt...")
    !apt-get install -y chromium-chromedriver

print("Browser and Driver installation attempt complete.")
# ----------------------------------------------------

# Python Imports (after installations)
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import re
import json
import pytz
import traceback
import gspread
from google.colab import auth # For user authentication
from google.auth import default # To get default credentials after auth
from google.auth import exceptions as google_auth_exceptions
import google.generativeai as genai

# --- Selenium Imports ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException

print("All libraries imported.")

# Cell 2: Configuration with Colab Forms

# --- General Configuration ---
VIETSTOCK_START_URL = "https://vietstock.vn/chung-khoan.htm"
BASE_URL = "https://vietstock.vn"
MAX_PAGES_TO_CRAWL = 5 #@param {type:"integer"} # Gi·∫£m ƒë·ªÉ test nhanh
HOURS_TO_CHECK = 72 #@param {type:"integer"} # Default to 3 days
PROGRAM_SIMILARITY_THRESHOLD = 85 #@param {type:"integer"} # Fuzzy match threshold for programs (0-100) - S·∫Ω d√πng l·∫°i

# --- Keyword Configuration ---
# KEYWORDS s·∫Ω bao g·ªìm c·∫£ t√™n c√¥ng ty v√† t√™n c√°c l√£nh ƒë·∫°o ch·ªß ch·ªët n·∫øu mu·ªën focus v√†o b√†i ph·ªèng v·∫•n CEO
KEYWORDS = {
    # Competitor Key: [List of keyword variations, including CEO names if desired]
    your_choice
    # Th√™m c√°c c√¥ng ty kh√°c n·∫øu c·∫ßn
}

# --- Google Sheet Configuration ---
GOOGLE_SHEET_ID = "your_choice" #@param {type:"string"}
TARGET_WORKSHEET_NAME = "your_choice" #@param {type:"string"}

# Map Competitor Key to the Google Sheet Tab name containing ITS OWN program list
# D√πng ƒë·ªÉ ƒë·ªçc danh s√°ch ch∆∞∆°ng tr√¨nh hi·ªán c√≥ c·ªßa t·ª´ng c√¥ng ty
COMPETITOR_PROGRAM_DB_SHEET_MAP = {
    your_choice
    # Th√™m c√°c c√¥ng ty kh√°c n·∫øu c√≥ sheet ch∆∞∆°ng tr√¨nh t∆∞∆°ng ·ª©ng
}

# --- NEW: Define the exact header order for the target sheet (vietstock_qu·∫£ng_c√°o) ---
SHEET_HEADER_OUTPUT = [
    your_choice]

# --- Gemini AI Configuration ---
GEMINI_API_KEY = "your_choice" #@param {type:"string"} # IMPORTANT: Replace with your actual key or use Colab Secrets

if not GEMINI_API_KEY or "YOUR_GEMINI_API_KEY" in GEMINI_API_KEY or len(GEMINI_API_KEY) < 15:
     print("‚ö†Ô∏è WARNING: Please set a valid Gemini API Key in the form above.")
     # raise ValueError("L·ªñI: Vui l√≤ng ƒë·∫∑t Gemini API Key h·ª£p l·ªá v√†o form c·∫•u h√¨nh.")

GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest' # Ho·∫∑c 'gemini-pro'

# --- Timezone Configuration ---
TARGET_TIMEZONE = 'Asia/Ho_Chi_Minh' # GMT+7

# --- Service Account Configuration for Google Sheets ---
# IMPORTANT: Upload your service account JSON key file to Colab or Google Drive
# and provide the correct path here.
_SERVICE_ACCOUNT_JSON_PATH_FORM = "your_choice" #@param {type:"string"}
SERVICE_ACCOUNT_JSON_PATH = None
if _SERVICE_ACCOUNT_JSON_PATH_FORM:
    SERVICE_ACCOUNT_JSON_PATH = _SERVICE_ACCOUNT_JSON_PATH_FORM
else:
    print("‚ö†Ô∏è WARNING: Service Account JSON path is not set. Google Sheets access will fail.")
    # raise ValueError("L·ªñI: Vui l√≤ng cung c·∫•p ƒë∆∞·ªùng d·∫´n ƒë·∫øn file JSON c·ªßa Service Account.")

# --- Other Constants ---
REQUESTS_TIMEOUT = 20
SELENIUM_WAIT = 15
SLEEP_BETWEEN_PAGES = 3
SLEEP_BETWEEN_AI_CALLS = 2 # Gi·ªØ kho·∫£ng ngh·ªâ quan tr·ªçng
SHEET_WRITE_BATCH_SIZE = 50
SHEET_WRITE_DELAY = 1
MAX_CONTENT_LENGTH_FOR_AI = 15000 # Gi·ªõi h·∫°n k√Ω t·ª± g·ª≠i cho AI ƒë·ªÉ tr√°nh qu√° d√†i

print("Configuration loaded.")
print(f"Target Sheet: {TARGET_WORKSHEET_NAME} in Sheet ID: {GOOGLE_SHEET_ID}")
print(f"Output Sheet Header: {SHEET_HEADER_OUTPUT}")
if SERVICE_ACCOUNT_JSON_PATH:
    print(f"Using Service Account Key: {SERVICE_ACCOUNT_JSON_PATH}")

# Cell 3: Helper Functions

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import re
import json
import pytz
import traceback
import gspread # S·∫Ω d√πng gspread.service_account
# from google.colab import auth # Kh√¥ng d√πng user auth n·ªØa
# from google.auth import default # Kh√¥ng d√πng default creds n·ªØa
from google.oauth2.service_account import Credentials # D√πng cho Service Account
import google.generativeai as genai
from fuzzywuzzy import fuzz, process as fuzzy_process # Th√™m process

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException
import os

# --- Time Function ---
def get_vietnam_time():
    return datetime.now(pytz.timezone(TARGET_TIMEZONE))

# --- Google Sheets Authentication with Service Account ---
def authenticate_google_sheets_service_account(json_keyfile_path: str) -> gspread.Client | None:
    """Authenticates using a Service Account JSON key file."""
    if not json_keyfile_path:
        print("‚ùå Critical Error: Service Account JSON path is missing.")
        return None
    if not os.path.exists(json_keyfile_path):
        print(f"‚ùå Critical Error: Service Account JSON file not found at: {json_keyfile_path}")
        print("   Please upload the file and provide the correct path in Cell 2.")
        return None
    try:
        print(f"üîë Authenticating Google Sheets using Service Account: {json_keyfile_path}...")
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive' # C√≥ th·ªÉ c·∫ßn quy·ªÅn drive n·∫øu t·∫°o sheet m·ªõi
        ]
        creds = Credentials.from_service_account_file(json_keyfile_path, scopes=scopes)
        gc = gspread.authorize(creds)
        print("‚úÖ Google Sheets authentication with Service Account successful.")
        # Test connection by listing spreadsheets (optional)
        # gc.list_spreadsheet_files()
        return gc
    except Exception as e:
        print(f"‚ùå Critical Error: Failed Google Sheets Service Account authentication: {e}")
        if "invalid_grant" in str(e).lower() or "permission denied" in str(e).lower():
             print("   Hint: Check if the Service Account has 'Editor' access to the Google Sheet.")
             print("   Also, ensure the 'Google Sheets API' and 'Google Drive API' are enabled in your GCP project.")
        traceback.print_exc()
        return None

# --- Selenium Setup (Gi·ªØ nguy√™n ho·∫∑c ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn) ---
def setup_selenium_driver():
    # ... (Code setup_selenium_driver gi·ªØ nguy√™n nh∆∞ phi√™n b·∫£n g·ªëc c·ªßa b·∫°n) ...
    print("Setting up Selenium WebDriver...")
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-extensions")
    options.add_argument("--proxy-server='direct://'")
    options.add_argument("--proxy-bypass-list=*")
    options.add_argument("--start-maximized")

    chromedriver_path = "/usr/local/bin/chromedriver"
    if not os.path.exists(chromedriver_path):
        chromedriver_path = "/usr/bin/chromedriver"

    if not os.path.exists(chromedriver_path):
         print(f"ERROR: ChromeDriver executable not found at default paths.")
         raise FileNotFoundError("ChromeDriver executable not found.")

    print(f"Using ChromeDriver at: {chromedriver_path}")
    try:
        service = Service(executable_path=chromedriver_path)
        driver = webdriver.Chrome(service=service, options=options)
        print("WebDriver setup successful.")
        return driver
    except WebDriverException as e:
        print(f"WebDriver setup failed: {e}")
        raise
    except Exception as general_err:
         print(f"An unexpected error occurred during WebDriver setup: {general_err}")
         raise

# --- Read Program Databases from Sheets ---
def read_all_competitor_program_dbs(gc: gspread.Client, sheet_id: str, db_map: dict) -> dict:
    """
    Reads program names and details from ALL specified competitor program sheets.
    Returns a dict: { "CompetitorKey": [{"name": "Prog Name", "details": "Details"}, ...], ... }
    """
    print("Reading all competitor program databases from Google Sheets...")
    all_programs_data = {}
    if not gc:
        print("Warning: Google Sheets client not available. Cannot read program databases.")
        return all_programs_data

    try:
        spreadsheet = gc.open_by_key(sheet_id)
    except Exception as e:
        print(f"L·ªñI: Cannot access Google Sheet (ID: {sheet_id}) to read program DBs: {e}")
        return all_programs_data

    for competitor_key, sheet_name in db_map.items():
        all_programs_data[competitor_key] = []
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            all_values = worksheet.get_all_values()
            if not all_values or len(all_values) < 2:
                print(f"   Warning: Program DB sheet '{sheet_name}' for {competitor_key} is empty.")
                continue

            header_row = all_values[0]
            data_rows = all_values[1:]
            # Gi·∫£ s·ª≠ c·ªôt 'T√™n' l√† c·ªôt A (index 0) v√† 'ƒê·∫∑c ƒëi·ªÉm' l√† c·ªôt B (index 1)
            # Ho·∫∑c b·∫°n c√≥ th·ªÉ t√¨m index ƒë·ªông nh∆∞ tr∆∞·ªõc
            try: name_col_idx = header_row.index("T√™n") # Ho·∫∑c t√™n c·ªôt ch·ª©a t√™n ch∆∞∆°ng tr√¨nh
            except ValueError: name_col_idx = 0; print(f"   Warning: 'T√™n' header not found in '{sheet_name}', using column A.")
            try: details_col_idx = header_row.index("ƒê·∫∑c ƒëi·ªÉm") # Ho·∫∑c t√™n c·ªôt ch·ª©a m√¥ t·∫£
            except ValueError: details_col_idx = 1; print(f"   Warning: 'ƒê·∫∑c ƒëi·ªÉm' header not found in '{sheet_name}', using column B.")

            print(f"   Reading program DB for {competitor_key} from sheet '{sheet_name}'...")
            for row in data_rows:
                name = str(row[name_col_idx]).strip() if len(row) > name_col_idx and row[name_col_idx] else ""
                details = str(row[details_col_idx]).strip() if len(row) > details_col_idx and row[details_col_idx] else ""
                if name:
                    all_programs_data[competitor_key].append({"name": name, "details": details})
            print(f"   -> Stored {len(all_programs_data[competitor_key])} programs for {competitor_key}.")
        except gspread.exceptions.WorksheetNotFound:
            print(f"   Warning: Program DB sheet '{sheet_name}' not found for {competitor_key}. Skipping.")
        except Exception as e:
            print(f"   Error reading program DB sheet '{sheet_name}' for {competitor_key}: {e}")
    print("Finished reading all competitor program databases.")
    return all_programs_data

# --- Gemini AI Setup (Gi·ªØ nguy√™n) ---
def setup_gemini(api_key: str) -> genai.GenerativeModel | None:
    # ... (Code setup_gemini gi·ªØ nguy√™n nh∆∞ phi√™n b·∫£n g·ªëc c·ªßa b·∫°n) ...
    print(f"Configuring Gemini AI with model '{GEMINI_MODEL_NAME}'...")
    if not api_key or "YOUR_API_KEY" in api_key or len(api_key) < 15:
         print("ERROR: Invalid or missing Gemini API Key provided in configuration.")
         return None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        print("Gemini AI configured successfully.")
        return model
    except Exception as e:
        print(f"L·ªñI: Error configuring Gemini AI: {e}")
    return None

# --- NEW: Gemini Analysis Function (AI Call 1) ---
def analyze_article_with_gemini_v2(
    model: genai.GenerativeModel,
    title: str,
    content: str, # Full content, will be truncated
    competitor_being_analyzed: str, # T√™n c√¥ng ty ƒëang ƒë∆∞·ª£c t·∫≠p trung ph√¢n t√≠ch
    all_keywords_found_in_article: list # List c√°c keyword c·ª• th·ªÉ t√¨m th·∫•y trong b√†i
) -> dict:
    """
    Analyzes article content using Gemini AI.
    Focuses on the competitor_being_analyzed.
    """
    print(f"  [AI Analysis] Analyzing for '{competitor_being_analyzed}': '{title[:50]}...'")
    # C√°c tr∆∞·ªùng output mong mu·ªën t·ª´ AI
    default_result = {
        "is_advertising": "Kh√¥ng x√°c ƒë·ªãnh", # PR Ch∆∞∆°ng tr√¨nh/PR Th∆∞∆°ng hi·ªáu/Kh√¥ng Qu·∫£ng C√°o
        "promoted_program_product_name_ai": None, # T√™n SP/CT AI tr√≠ch xu·∫•t
        "main_message_ai": "L·ªói AI",
        "analysis_summary_llm_ai": "L·ªói AI",
        "article_focus_ai": "Kh√¥ng x√°c ƒë·ªãnh", # S·∫£n ph·∫©m/Th∆∞∆°ng hi·ªáu/Th·ªã tr∆∞·ªùng/L√£nh ƒë·∫°o
        "detailed_classification_ai": "Kh√¥ng x√°c ƒë·ªãnh",
        "tone_ai": "Kh√¥ng x√°c ƒë·ªãnh", # T√≠ch c·ª±c/Trung l·∫≠p/Ti√™u c·ª±c/Qu·∫£ng b√°
        "_error": True
    }
    if not model:
        return {**default_result, "analysis_summary_llm_ai": "L·ªói Model AI"}

    truncated_content = content[:MAX_CONTENT_LENGTH_FOR_AI]
    if not truncated_content or len(truncated_content) < 100: # C·∫ßn ƒë·ªß n·ªôi dung
        return {**default_result, "analysis_summary_llm_ai": "N·ªôi dung kh√¥ng ƒë·ªß ƒë·ªÉ ph√¢n t√≠ch"}

    # Nh·∫•n m·∫°nh vi·ªác ph√¢n t√≠ch k·ªπ n·∫øu c√≥ keyword
    keyword_emphasis = ""
    if competitor_being_analyzed.lower() in [kw.lower() for kw in all_keywords_found_in_article]:
        keyword_emphasis = f"B√†i vi·∫øt n√†y c√≥ ch·ª©a t·ª´ kh√≥a tr·ª±c ti·∫øp li√™n quan ƒë·∫øn '{competitor_being_analyzed}' (v√≠ d·ª•: {', '.join(all_keywords_found_in_article)}), v√¨ v·∫≠y h√£y ph√¢n t√≠ch k·ªπ l∆∞·ª°ng kh·∫£ nƒÉng ƒë√¢y l√† m·ªôt b√†i PR cho c√¥ng ty n√†y ho·∫∑c s·∫£n ph·∫©m/d·ªãch v·ª• c·ªßa h·ªç, ngay c·∫£ khi ch·ªâ ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p m·ªôt c√°ch tinh t·∫ø."
    elif any(ceo_kw.lower() in title.lower() or ceo_kw.lower() in truncated_content.lower() for ceo_kw in KEYWORDS.get(competitor_being_analyzed, [])):
        keyword_emphasis = f"B√†i vi·∫øt n√†y c√≥ th·ªÉ li√™n quan ƒë·∫øn l√£nh ƒë·∫°o c·ªßa '{competitor_being_analyzed}'. H√£y xem x√©t kh·∫£ nƒÉng ƒë√¢y l√† m·ªôt b√†i PR th∆∞∆°ng hi·ªáu th√¥ng qua ph·ªèng v·∫•n ho·∫∑c ph√°t bi·ªÉu c·ªßa l√£nh ƒë·∫°o."


    prompt = f"""
    B·∫°n l√† m·ªôt chuy√™n vi√™n ph√¢n t√≠ch truy·ªÅn th√¥ng ng√†nh t√†i ch√≠nh - ch·ª©ng kho√°n.
    Ph√¢n t√≠ch b√†i b√°o ti·∫øng Vi·ªát sau ƒë√¢y, t·∫≠p trung ƒë√°nh gi√° v·ªÅ c√¥ng ty "{competitor_being_analyzed}".

    Ti√™u ƒë·ªÅ: {title}
    N·ªôi dung (tr√≠ch ƒëo·∫°n): {truncated_content}
    {keyword_emphasis}

    D·ª±a **CH·ªà** v√†o n·ªôi dung b√†i b√°o ƒë∆∞·ª£c cung c·∫•p, h√£y tr·∫£ l·ªùi c√°c c√¢u h·ªèi sau d∆∞·ªõi d·∫°ng m·ªôt ƒë·ªëi t∆∞·ª£ng JSON:

    1.  `is_advertising`: (String) Ph√¢n lo·∫°i b√†i vi·∫øt n√†y li√™n quan ƒë·∫øn "{competitor_being_analyzed}" v√†o M·ªòT trong ba nh√≥m ch√≠nh:
        *   "PR Ch∆∞∆°ng tr√¨nh/S·∫£n ph·∫©m": N·∫øu n·ªôi dung ch√≠nh l√† qu·∫£ng b√°, gi·ªõi thi·ªáu m·ªôt s·∫£n ph·∫©m, d·ªãch v·ª•, ch∆∞∆°ng tr√¨nh ∆∞u ƒë√£i ho·∫∑c s·ª± ki·ªán c·ª• th·ªÉ c·ªßa "{competitor_being_analyzed}".
        *   "PR Th∆∞∆°ng hi·ªáu": N·∫øu n·ªôi dung ch√≠nh l√† x√¢y d·ª±ng h√¨nh ·∫£nh, uy t√≠n, gi·ªõi thi·ªáu v·ªÅ c√¥ng ty "{competitor_being_analyzed}", l√£nh ƒë·∫°o, gi·∫£i th∆∞·ªüng, ho·∫°t ƒë·ªông CSR, b√°o c√°o th∆∞·ªùng ni√™n, ho·∫∑c ph√¢n t√≠ch th·ªã tr∆∞·ªùng do c√¥ng ty ph√°t h√†nh m√† KH√îNG t·∫≠p trung qu·∫£ng b√° m·ªôt ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m c·ª• th·ªÉ n√†o. Bao g·ªìm c·∫£ c√°c b√†i ph·ªèng v·∫•n CEO/l√£nh ƒë·∫°o.
        *   "Kh√¥ng Qu·∫£ng C√°o": N·∫øu n·ªôi dung l√† tin t·ª©c th·ªã tr∆∞·ªùng chung kh√¥ng do "{competitor_being_analyzed}" t·ª± PR, ph√¢n t√≠ch vƒ© m√¥, th√¥ng tin v·ªÅ c√°c c√¥ng ty kh√°c, ho·∫∑c kh√¥ng nh·∫±m m·ª•c ƒë√≠ch qu·∫£ng b√° cho "{competitor_being_analyzed}".

    2.  `promoted_program_product_name_ai`: (String ho·∫∑c null) N·∫øu `is_advertising` l√† "PR Ch∆∞∆°ng tr√¨nh/S·∫£n ph·∫©m", h√£y tr√≠ch xu·∫•t t√™n c·ª• th·ªÉ c·ªßa ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m/d·ªãch v·ª• ƒë∆∞·ª£c qu·∫£ng b√°. N·∫øu kh√¥ng, ƒë·ªÉ l√† null. C·ªë g·∫Øng tr√≠ch xu·∫•t t√™n ƒë·∫ßy ƒë·ªß v√† ch√≠nh x√°c nh·∫•t c√≥ th·ªÉ.

    3.  `main_message_ai`: (String) Th√¥ng ƒëi·ªáp ch√≠nh ho·∫∑c ƒëi·ªÉm c·ªët l√µi c·ªßa b√†i vi·∫øt li√™n quan ƒë·∫øn "{competitor_being_analyzed}" l√† g√¨? T√≥m t·∫Øt r·∫•t ng·∫Øn g·ªçn (1-2 c√¢u).

    4.  `analysis_summary_llm_ai`: (String) T√≥m t·∫Øt ph√¢n t√≠ch s√¢u h∆°n v·ªÅ m·ª•c ƒë√≠ch ho·∫∑c h√†m √Ω ch√≠nh c·ªßa b√†i vi·∫øt ƒë·ªëi v·ªõi "{competitor_being_analyzed}" (v√≠ d·ª•: "B√†i vi·∫øt nh·∫±m thu h√∫t kh√°ch h√†ng m·ªõi cho s·∫£n ph·∫©m X", "B√†i vi·∫øt kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø d·∫´n ƒë·∫ßu c·ªßa c√¥ng ty Y tr√™n th·ªã tr∆∞·ªùng Z", "B√†i vi·∫øt cung c·∫•p g√≥c nh√¨n c·ªßa chuy√™n gia t·ª´ c√¥ng ty A v·ªÅ tri·ªÉn v·ªçng ng√†nh"). (2-3 c√¢u)

    5.  `article_focus_ai`: (String) Tr·ªçng t√¢m ch√≠nh c·ªßa b√†i vi·∫øt l√† g√¨? Ch·ªçn M·ªòT trong c√°c gi√° tr·ªã: "S·∫£n ph·∫©m/D·ªãch v·ª•", "Th∆∞∆°ng hi·ªáu c√¥ng ty", "L√£nh ƒë·∫°o c√¥ng ty", "Ph√¢n t√≠ch/Nh·∫≠n ƒë·ªãnh th·ªã tr∆∞·ªùng", "Tin t·ª©c chung", "Kh√°c".

    6.  `detailed_classification_ai`: (String) D·ª±a v√†o c√°c ph√¢n t√≠ch tr√™n, h√£y ƒë∆∞a ra m·ªôt ph√¢n lo·∫°i chi ti·∫øt h∆°n. V√≠ d·ª•: "PR S·∫£n ph·∫©m hi·ªán c√≥", "PR S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng", "PR Th∆∞∆°ng hi·ªáu - Ph·ªèng v·∫•n CEO", "PR Th∆∞∆°ng hi·ªáu - Gi·∫£i th∆∞·ªüng", "PR Th∆∞∆°ng hi·ªáu - CSR", "Nh·∫≠n ƒë·ªãnh th·ªã tr∆∞·ªùng t·ª´ chuy√™n gia c√¥ng ty", "Tin t·ª©c ho·∫°t ƒë·ªông c√¥ng ty", "Kh√¥ng li√™n quan".

    7.  `tone_ai`: (String) Gi·ªçng ƒëi·ªáu chung c·ªßa b√†i vi·∫øt khi n√≥i v·ªÅ "{competitor_being_analyzed}" ho·∫∑c s·∫£n ph·∫©m/ch·ªß ƒë·ªÅ li√™n quan l√† g√¨? Ch·ªçn M·ªòT: "T√≠ch c·ª±c", "Ti√™u c·ª±c", "Trung l·∫≠p", "Qu·∫£ng b√°/Ca ng·ª£i", "C·∫£nh b√°o/Ch√™ bai".

    **Y√™u c·∫ßu ƒë·ªãnh d·∫°ng:** Tr·∫£ v·ªÅ m·ªôt ƒë·ªëi t∆∞·ª£ng JSON **DUY NH·∫§T** v√† **H·ª¢P L·ªÜ**.
    **KH√îNG** bao g·ªìm b·∫•t k·ª≥ gi·∫£i th√≠ch n√†o b√™n ngo√†i c·∫•u tr√∫c JSON. **KH√îNG** s·ª≠ d·ª•ng markdown code blocks.

    V√≠ d·ª• JSON (n·∫øu PR s·∫£n ph·∫©m):
    {{
        "is_advertising": "PR Ch∆∞∆°ng tr√¨nh/S·∫£n ph·∫©m",
        "promoted_program_product_name_ai": "G√≥i vay Margin Super X",
        "main_message_ai": "{competitor_being_analyzed} ra m·∫Øt g√≥i vay margin Super X v·ªõi l√£i su·∫•t h·∫•p d·∫´n.",
        "analysis_summary_llm_ai": "B√†i vi·∫øt t·∫≠p trung gi·ªõi thi·ªáu l·ª£i √≠ch v√† c√°ch th·ª©c tham gia ch∆∞∆°ng tr√¨nh vay margin m·ªõi c·ªßa {competitor_being_analyzed} nh·∫±m thu h√∫t nh√† ƒë·∫ßu t∆∞.",
        "article_focus_ai": "S·∫£n ph·∫©m/D·ªãch v·ª•",
        "detailed_classification_ai": "PR S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng",
        "tone_ai": "Qu·∫£ng b√°/Ca ng·ª£i"
    }}
    """
    safety_settings = [ # Gi·ªØ nguy√™n safety settings
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    generation_config = genai.types.GenerationConfig(
        temperature=0.3, # C√≥ th·ªÉ tƒÉng nh·∫π ƒë·ªÉ AI linh ho·∫°t h∆°n trong ph√¢n lo·∫°i
        response_mime_type="application/json" # Y√™u c·∫ßu Gemini tr·∫£ v·ªÅ JSON
    )

    try:
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        time.sleep(SLEEP_BETWEEN_AI_CALLS)

        ai_result_text = response.text
        # print(f"Raw AI response for {competitor_being_analyzed}: {ai_result_text[:300]}") # Debug

        try:
            ai_data = json.loads(ai_result_text)
        except json.JSONDecodeError:
            # Th·ª≠ tr√≠ch xu·∫•t JSON t·ª´ markdown n·∫øu c√≥
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_result_text, re.DOTALL)
            if json_match:
                cleaned_text = json_match.group(1)
                ai_data = json.loads(cleaned_text)
            else: # N·∫øu kh√¥ng c√≥ markdown, th·ª≠ t√¨m JSON tr·ª±c ti·∫øp
                json_match_direct = re.search(r'(\{.*?\})', ai_result_text, re.DOTALL)
                if json_match_direct:
                    ai_data = json.loads(json_match_direct.group(1))
                else:
                    print(f"  [AI Analysis] L·ªói: Gemini response kh√¥ng ph·∫£i JSON h·ª£p l·ªá v√† kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c. Response: {ai_result_text[:300]}")
                    return {**default_result, "analysis_summary_llm_ai": f"L·ªói gi·∫£i m√£ JSON: {ai_result_text[:200]}"}

        # Ki·ªÉm tra c√°c kh√≥a b·∫Øt bu·ªôc
        required_keys = default_result.keys() - {"_error"} # Tr·ª´ _error ra kh·ªèi c√°c key c·∫ßn check
        if not all(key in ai_data for key in required_keys):
            missing_keys = [key for key in required_keys if key not in ai_data]
            print(f"  [AI Analysis] L·ªói: K·∫øt qu·∫£ AI thi·∫øu c√°c kh√≥a b·∫Øt bu·ªôc: {missing_keys}. Parsed: {ai_data}")
            # C·ªë g·∫Øng ƒëi·ªÅn c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh cho c√°c kh√≥a b·ªã thi·∫øu
            for key in missing_keys:
                ai_data[key] = default_result[key] # G√°n gi√° tr·ªã m·∫∑c ƒë·ªãnh
            # return {**default_result, "analysis_summary_llm_ai": f"L·ªói thi·∫øu kh√≥a JSON: {str(ai_data)[:200]}"}

        ai_data["_error"] = False
        print(f"  [AI Analysis] Ph√¢n t√≠ch cho '{competitor_being_analyzed}' th√†nh c√¥ng. Lo·∫°i QC: {ai_data.get('is_advertising')}")
        return ai_data

    except Exception as e:
        print(f"  [AI Analysis] L·ªói khi g·ªçi Gemini cho '{competitor_being_analyzed}': {e}")
        # traceback.print_exc(limit=1)
        error_summary = f"L·ªói Exception AI: {str(e)[:100]}"
        if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback') and e.response.prompt_feedback:
            error_summary += f" (Feedback: {e.response.prompt_feedback})"
        return {**default_result, "analysis_summary_llm_ai": error_summary}


# --- Time Parsing, Fetch Article Details, Find Keywords (Gi·ªØ nguy√™n) ---
def parse_article_time(date_text: str, current_time_utc: datetime) -> datetime | None:
    # ... (Code parse_article_time gi·ªØ nguy√™n) ...
    if not date_text: return None
    now_utc = current_time_utc
    local_tz = pytz.timezone(TARGET_TIMEZONE)
    now_local = now_utc.astimezone(local_tz)
    article_time_naive = None; parsed_format = None
    match = re.fullmatch(r'(\d{1,2})/(\d{1,2})\s+(\d{1,2}):(\d{1,2})', date_text)
    if match:
        try:
            day, month, hour, minute = map(int, match.groups()); year = now_local.year
            article_time_naive = datetime(year, month, day, hour, minute)
            if local_tz.localize(article_time_naive) > now_local + timedelta(hours=1): article_time_naive = article_time_naive.replace(year=year - 1)
            parsed_format = "DD/MM HH:MM"
        except ValueError: pass
    if not parsed_format:
        match = re.fullmatch(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{1,2})', date_text)
        if match:
            try:
                day, month, year, hour, minute = map(int, match.groups())
                if 1990 < year < 2100: article_time_naive = datetime(year, month, day, hour, minute); parsed_format = "DD/MM/YYYY HH:MM"
            except ValueError: pass
    if article_time_naive and parsed_format:
        try: return local_tz.localize(article_time_naive, is_dst=None).astimezone(pytz.utc)
        except Exception: return None
    for rel_pattern, unit, delta_func in [
        (r'(\d+)\s*(ph√∫t|phut|p|minute|min)s?\s*(tr∆∞·ªõc|truoc|ago)', 'minutes', timedelta),
        (r'(\d+)\s*(gi·ªù|gio|g|h|hour)s?\s*(tr∆∞·ªõc|truoc|ago)', 'hours', timedelta),
        (r'(\d+)\s*(ng√†y|ngay|d|day)s?\s*(tr∆∞·ªõc|truoc|ago)', 'days', timedelta)
    ]:
        match = re.fullmatch(rel_pattern, date_text, re.IGNORECASE)
        if match:
            try: return now_utc - delta_func(**{unit: int(match.group(1))})
            except ValueError: pass
    try:
        article_time_aware = datetime.fromisoformat(date_text.replace(" ", "T"))
        if article_time_aware.tzinfo is None or article_time_aware.tzinfo.utcoffset(article_time_aware) is None:
            article_time_aware = local_tz.localize(article_time_aware)
        return article_time_aware.astimezone(pytz.utc)
    except Exception: pass
    return None

def fetch_article_details(article_url: str, start_crawl_time_utc: datetime) -> tuple[str, str | None, datetime | None]:
    # ... (Code fetch_article_details gi·ªØ nguy√™n, ƒë·∫£m b·∫£o tr·∫£ v·ªÅ full content) ...
    content = "L·ªói t·∫£i n·ªôi dung"; date_text_detail = None; article_time_utc = None
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(article_url, headers=headers, timeout=REQUESTS_TIMEOUT, allow_redirects=True)
        response.raise_for_status()
        try: response.encoding = response.apparent_encoding; html_content = response.text
        except Exception: response.encoding = 'utf-8'; html_content = response.text
        detail_soup = BeautifulSoup(html_content, 'html.parser')
        time_selectors = ["meta[property='article:published_time']", "meta[itemprop='datePublished']", ".date", ".time", ".article__meta > time", "span.source-date", "time[datetime]"]
        for selector in time_selectors:
            el = detail_soup.select_one(selector)
            if el:
                dt_text = el.get('content') or el.get('datetime') or el.get_text(strip=True)
                if dt_text: date_text_detail = dt_text.strip(); article_time_utc = parse_article_time(date_text_detail, start_crawl_time_utc); break
        content_selectors = ["div[itemprop='articleBody']", "#vst_detail", ".content-news", ".article__body", ".detail-content", "article.content", "div.article-content"]
        content_area = next((detail_soup.select_one(s) for s in content_selectors if detail_soup.select_one(s)), None)
        if content_area:
            for el_to_remove in content_area.select('script, style, iframe, .adsbygoogle, .related-posts, figure.op-interactive, aside'): el_to_remove.decompose()
            # L·∫•y to√†n b·ªô text, bao g·ªìm c·∫£ heading, list items,...
            # content = content_area.get_text(separator="\n", strip=True)
            # Ho·∫∑c chi ti·∫øt h∆°n:
            text_parts = []
            for element in content_area.find_all(True, recursive=True): # L·∫•y t·∫•t c·∫£ c√°c th·∫ª con
                if element.name in ['script', 'style', 'noscript', 'iframe', 'a', 'button']: continue # B·ªè qua c√°c th·∫ª kh√¥ng mong mu·ªën
                text = element.string # L·∫•y text tr·ª±c ti·∫øp c·ªßa th·∫ª (kh√¥ng bao g·ªìm con)
                if text and text.strip():
                    text_parts.append(text.strip())
            content = "\n".join(text_parts)
            content = re.sub(r'\n\s*\n', '\n', content).strip()
            if not content or len(content) < 100 : # Fallback n·∫øu c√°ch tr√™n kh√¥ng hi·ªáu qu·∫£
                 content = content_area.get_text(separator="\n", strip=True)
                 content = re.sub(r'\n\s*\n', '\n', content).strip()

        else: content = "Kh√¥ng t√¨m th·∫•y n·ªôi dung"
    except requests.exceptions.HTTPError as http_err: content = f"L·ªói t·∫£i n·ªôi dung ({http_err.response.status_code})"
    except requests.exceptions.RequestException as req_err: content = f"L·ªói t·∫£i n·ªôi dung ({type(req_err).__name__})"
    except Exception as detail_err: content = f"L·ªói x·ª≠ l√Ω n·ªôi dung ({type(detail_err).__name__})"
    return content, date_text_detail, article_time_utc


def find_keywords_and_competitors(text_lower: str, title_lower: str) -> tuple[set, list]:
    # ... (Code find_keywords_and_competitors gi·ªØ nguy√™n) ...
    matched_competitors_keys = set() # Set c√°c key c·ªßa ƒë·ªëi th·ªß (VPS, SSI)
    matched_keywords_list = [] # List c√°c t·ª´ kh√≥a c·ª• th·ªÉ t√¨m th·∫•y
    combined_text = title_lower + "\n" + text_lower
    for comp_key, kws_list in KEYWORDS.items():
        for kw in kws_list:
            pattern = r'\b' + re.escape(kw.lower()) + r'\b'
            if re.search(pattern, combined_text):
                matched_competitors_keys.add(comp_key)
                if kw not in matched_keywords_list: # Ch·ªâ th√™m keyword c·ª• th·ªÉ m·ªôt l·∫ßn
                     matched_keywords_list.append(kw)
    return matched_competitors_keys, sorted(list(set(matched_keywords_list))) # ƒê·∫£m b·∫£o unique v√† sorted

# --- Fuzzy Match Program Name with Database ---
def match_program_with_db(
    program_name_from_ai: str,
    competitor_key: str, # e.g. "VPS"
    program_databases: dict, # Dict ch·ª©a DB c·ªßa t·∫•t c·∫£ c√°c c√¥ng ty
    threshold: int = PROGRAM_SIMILARITY_THRESHOLD
) -> tuple[str | None, str]: # (matched_db_program_name, status_message)
    """
    Matches AI-extracted program name against a specific competitor's program database.
    """
    if not program_name_from_ai or not competitor_key:
        return None, "Thi·∫øu t√™n ch∆∞∆°ng tr√¨nh t·ª´ AI ho·∫∑c t√™n c√¥ng ty."

    competitor_db = program_databases.get(competitor_key)
    if not competitor_db:
        return program_name_from_ai, f"S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng (Kh√¥ng c√≥ DB cho {competitor_key})"

    program_names_in_db = [p["name"] for p in competitor_db]
    if not program_names_in_db:
        return program_name_from_ai, f"S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng (DB c·ªßa {competitor_key} r·ªóng)"

    # fuzzywuzzy.process.extractOne returns (best_match, score)
    best_match = fuzzy_process.extractOne(program_name_from_ai, program_names_in_db, scorer=fuzz.token_set_ratio)

    if best_match and best_match[1] >= threshold:
        # print(f"    Fuzzy match for '{program_name_from_ai}' ({competitor_key}): Found '{best_match[0]}' with score {best_match[1]}%")
        return best_match[0], f"Kh·ªõp v·ªõi DB ({best_match[1]}%)" # Tr·∫£ v·ªÅ t√™n ch∆∞∆°ng tr√¨nh trong DB
    else:
        # print(f"    Fuzzy match for '{program_name_from_ai}' ({competitor_key}): No good match found (Best: {best_match[0]} at {best_match[1]}%). Potential new program.")
        return program_name_from_ai, "S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng (C·∫ßn review)"

print("Helper functions defined.")

# Cell 4: Core Processing Logic

def process_single_article_for_competitor(
    article_title: str,
    article_url: str,
    full_content: str, # N·ªôi dung ƒë·∫ßy ƒë·ªß c·ªßa b√†i vi·∫øt
    competitor_key_being_analyzed: str, # Key c·ªßa c√¥ng ty ƒëang ƒë∆∞·ª£c ph√¢n t√≠ch (e.g., "VPS")
    all_keywords_found_in_article: list, # List t·∫•t c·∫£ keyword t√¨m th·∫•y trong b√†i
    crawl_time_str_local: str,
    gemini_model: genai.GenerativeModel,
    all_program_dbs: dict # Database ch∆∞∆°ng tr√¨nh c·ªßa t·∫•t c·∫£ c√°c c√¥ng ty
) -> dict | None:
    """
    Processes a single article focusing on one competitor.
    Performs AI analysis and then matches program name with DB.
    Returns a dictionary ready for the output sheet.
    """
    print(f"-> Processing article '{article_title[:60]}' for competitor '{competitor_key_being_analyzed}'")

    # --- AI Analysis (Call 1) ---
    ai_analysis_result = analyze_article_with_gemini_v2(
        gemini_model,
        article_title,
        full_content, # Truy·ªÅn full content, AI function s·∫Ω t·ª± truncate
        competitor_key_being_analyzed,
        all_keywords_found_in_article
    )

    # --- Prepare data for the output sheet ---
    output_row = {header: None for header in SHEET_HEADER_OUTPUT} # Kh·ªüi t·∫°o v·ªõi None

    output_row['Th·ªùi gian c·∫≠p nh·∫≠t'] = crawl_time_str_local
    output_row['Ti√™u ƒë·ªÅ b√†i vi·∫øt'] = article_title
    output_row['C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch'] = competitor_key_being_analyzed
    # 'L√† c√¥ng ty ƒë·ªëi th·ªß?' s·∫Ω ƒë∆∞·ª£c x√°c ƒë·ªãnh trong h√†m crawl_vietstock d·ª±a tr√™n KEYWORDS
    output_row['URL'] = article_url
    output_row['N·ªôi dung g·ªëc'] = full_content[:MAX_CONTENT_LENGTH_FOR_AI] # L∆∞u tr√≠ch ƒëo·∫°n ho·∫∑c full n·∫øu mu·ªën

    if ai_analysis_result.get("_error"):
        output_row['L√† qu·∫£ng c√°o? (AI)'] = "L·ªói AI"
        output_row['Th√¥ng ƒëi·ªáp ch√≠nh (AI)'] = ai_analysis_result.get("main_message_ai", "L·ªói AI")
        output_row['T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)'] = ai_analysis_result.get("analysis_summary_llm_ai", "L·ªói AI")
        output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "L·ªói ph√¢n t√≠ch AI"
        output_row['Tr·ªçng t√¢m b√†i vi·∫øt (AI)'] = "L·ªói AI"
        output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = "L·ªói AI"
        output_row['Gi·ªçng ƒëi·ªáu (AI)'] = "L·ªói AI"
        return output_row

    # ƒêi·ªÅn c√°c tr∆∞·ªùng t·ª´ k·∫øt qu·∫£ AI
    output_row['L√† qu·∫£ng c√°o? (AI)'] = ai_analysis_result.get("is_advertising")
    output_row['Th√¥ng ƒëi·ªáp ch√≠nh (AI)'] = ai_analysis_result.get("main_message_ai")
    output_row['T√≥m t·∫Øt ph√¢n t√≠ch LLM (AI)'] = ai_analysis_result.get("analysis_summary_llm_ai")
    output_row['Tr·ªçng t√¢m b√†i vi·∫øt (AI)'] = ai_analysis_result.get("article_focus_ai")
    output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = ai_analysis_result.get("detailed_classification_ai") # S·∫Ω c·∫≠p nh·∫≠t sau n·∫øu l√† SP
    output_row['Gi·ªçng ƒëi·ªáu (AI)'] = ai_analysis_result.get("tone_ai")
    output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "ƒê√£ x·ª≠ l√Ω AI" # Tr·∫°ng th√°i ban ƒë·∫ßu

    program_name_ai = ai_analysis_result.get("promoted_program_product_name_ai")
    is_program_pr_ai = output_row['L√† qu·∫£ng c√°o? (AI)'] == "PR Ch∆∞∆°ng tr√¨nh/S·∫£n ph·∫©m"

    final_program_name_display = None
    program_status_message = ""

    if is_program_pr_ai and program_name_ai:
        matched_db_name, status_msg = match_program_with_db(
            program_name_ai,
            competitor_key_being_analyzed,
            all_program_dbs # Truy·ªÅn DB c·ªßa t·∫•t c·∫£ c√¥ng ty
        )
        program_status_message = status_msg

        if "Kh·ªõp v·ªõi DB" in status_msg:
            final_program_name_display = matched_db_name # Hi·ªÉn th·ªã t√™n t·ª´ DB n·∫øu kh·ªõp
            output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = f"PR S·∫£n ph·∫©m hi·ªán c√≥ ({competitor_key_being_analyzed})"
            output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "ƒê√£ x·ª≠ l√Ω - SP kh·ªõp DB"
        elif "S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng" in status_msg:
            final_program_name_display = program_name_ai # Hi·ªÉn th·ªã t√™n t·ª´ AI
            output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = f"PR S·∫£n ph·∫©m m·ªõi ti·ªÅm nƒÉng ({competitor_key_being_analyzed})"
            output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "ƒê√£ x·ª≠ l√Ω - SP m·ªõi ti·ªÅm nƒÉng"
        else: # Tr∆∞·ªùng h·ª£p kh√°c
            final_program_name_display = program_name_ai
            output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = f"ƒê√£ x·ª≠ l√Ω - {status_msg}"

        output_row['T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn'] = final_program_name_display
        print(f"    Program for '{competitor_key_being_analyzed}': '{final_program_name_display}'. Status: {program_status_message}")

    elif is_program_pr_ai and not program_name_ai:
        output_row['T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn'] = "AI x√°c ƒë·ªãnh PR SP nh∆∞ng kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c t√™n"
        output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "L·ªói AI tr√≠ch xu·∫•t t√™n SP"
    # C√°c tr∆∞·ªùng h·ª£p kh√°c (PR Th∆∞∆°ng hi·ªáu, Kh√¥ng QC) th√¨ 'T√™n ch∆∞∆°ng tr√¨nh/s·∫£n ph·∫©m ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn' s·∫Ω l√† None

    # N·∫øu l√† PR Th∆∞∆°ng hi·ªáu, c·∫≠p nh·∫≠t Ph√¢n lo·∫°i chi ti·∫øt
    if output_row['L√† qu·∫£ng c√°o? (AI)'] == "PR Th∆∞∆°ng hi·ªáu":
        if "L√£nh ƒë·∫°o c√¥ng ty" in str(output_row['Tr·ªçng t√¢m b√†i vi·∫øt (AI)']):
            output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = f"PR Th∆∞∆°ng hi·ªáu - B√†i vi·∫øt v·ªÅ l√£nh ƒë·∫°o ({competitor_key_being_analyzed})"
        else:
            output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = f"PR Th∆∞∆°ng hi·ªáu ({competitor_key_being_analyzed})"
        output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "ƒê√£ x·ª≠ l√Ω - PR Th∆∞∆°ng hi·ªáu"


    if output_row['L√† qu·∫£ng c√°o? (AI)'] == "Kh√¥ng Qu·∫£ng C√°o":
        output_row['Ph√¢n lo·∫°i chi ti·∫øt (AI/DB)'] = f"Kh√¥ng qu·∫£ng c√°o ({competitor_key_being_analyzed})"
        output_row['Tr·∫°ng th√°i x·ª≠ l√Ω'] = "ƒê√£ x·ª≠ l√Ω - Kh√¥ng qu·∫£ng c√°o"

    return output_row


# --- Write to Google Sheet (Chung, d√πng l·∫°i h√†m write_data_to_sheet t·ª´ g·ª£i √Ω tr∆∞·ªõc) ---
def write_data_to_google_sheet(
    gc: gspread.Client,
    sheet_id: str,
    worksheet_name: str,
    header: list,
    data_rows_as_dicts: list
) -> bool:
    if not data_rows_as_dicts:
        print(f"No data to write to Google Sheet '{worksheet_name}'.")
        return True # Coi nh∆∞ th√†nh c√¥ng v√¨ kh√¥ng c√≥ g√¨ ƒë·ªÉ ghi
    if not gc:
        print(f"Error: Google Sheets client not available. Cannot write to '{worksheet_name}'.")
        return False

    print(f"\nPreparing {len(data_rows_as_dicts)} rows for Google Sheet '{worksheet_name}'...")
    rows_to_add_final = []
    for item_dict in data_rows_as_dicts:
        row_as_list = [item_dict.get(col_name, '') for col_name in header] # D√πng '' cho gi√° tr·ªã r·ªóng
        rows_to_add_final.append(row_as_list)

    if not rows_to_add_final:
        print(f"No final rows generated to write to '{worksheet_name}'.")
        return True

    print(f"Attempting to write {len(rows_to_add_final)} rows to '{worksheet_name}'...")
    try:
        spreadsheet = gc.open_by_key(sheet_id)
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            current_header_gs = []
            try:
                current_header_gs = worksheet.row_values(1)
            except Exception: # C√≥ th·ªÉ sheet m·ªõi ho√†n to√†n, ch∆∞a c√≥ d√≤ng n√†o
                pass

            if not current_header_gs or current_header_gs != header:
                print(f"   Updating header for sheet '{worksheet_name}'...")
                # X√≥a d·ªØ li·ªáu c≈© n·∫øu header thay ƒë·ªïi (t√πy ch·ªçn, c·∫©n th·∫≠n)
                # worksheet.clear()
                worksheet.update(range_name='A1', values=[header], value_input_option='USER_ENTERED')
                worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(header))}', {'textFormat': {'bold': True}})
                print(f"   Header updated/set for '{worksheet_name}'.")

        except gspread.exceptions.WorksheetNotFound:
            print(f"   Worksheet '{worksheet_name}' not found. Creating...")
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows="100", cols=len(header) + 5) # Th√™m c·ªôt d·ª± ph√≤ng
            worksheet.update(range_name='A1', values=[header], value_input_option='USER_ENTERED')
            worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(header))}', {'textFormat': {'bold': True}})
            print(f"   Worksheet '{worksheet_name}' created with header.")

        # Append data in batches
        num_rows_to_write = len(rows_to_add_final)
        for i in range(0, num_rows_to_write, SHEET_WRITE_BATCH_SIZE):
            batch = rows_to_add_final[i:min(i + SHEET_WRITE_BATCH_SIZE, num_rows_to_write)]
            print(f"   Writing batch {i // SHEET_WRITE_BATCH_SIZE + 1} ({len(batch)} rows) to '{worksheet_name}'...")
            worksheet.append_rows(batch, value_input_option='USER_ENTERED', table_range='A1')
            if num_rows_to_write > SHEET_WRITE_BATCH_SIZE and (i + SHEET_WRITE_BATCH_SIZE < num_rows_to_write):
                time.sleep(SHEET_WRITE_DELAY)

        print(f"‚úÖ Successfully wrote {len(rows_to_add_final)} rows to '{worksheet_name}'.")
        return True
    except gspread.exceptions.APIError as api_err:
        print(f"L·ªñI API Google Sheets khi ghi v√†o '{worksheet_name}': {api_err}")
        if 'PERMISSION_DENIED' in str(api_err):
             print("   Hint: Ensure the Service Account has 'Editor' permissions on this Google Sheet.")
        return False
    except Exception as sheet_err:
        print(f"L·ªñI kh√¥ng x√°c ƒë·ªãnh khi ghi v√†o Google Sheet '{worksheet_name}': {sheet_err}")
        traceback.print_exc()
        return False

# --- Main Crawling Function ---
def crawl_vietstock():
    start_crawl_time_local = get_vietnam_time()
    start_crawl_time_utc = start_crawl_time_local.astimezone(pytz.utc)
    crawl_time_str_local = start_crawl_time_local.strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{crawl_time_str_local} {start_crawl_time_local.tzname()}] B·∫Øt ƒë·∫ßu crawl Vietstock (Ki·ªÉm tra {HOURS_TO_CHECK} gi·ªù g·∫ßn nh·∫•t)...")

    time_limit_utc = start_crawl_time_utc - timedelta(hours=HOURS_TO_CHECK)

    # --- Initializations ---
    gc = None
    driver = None
    gemini_model = None
    all_program_databases = {} # { "VPS": [{"name":..., "details":...}], ... }
    all_output_data_for_sheet = [] # List of dicts, m·ªói dict l√† 1 d√≤ng cho sheet output
    all_articles_scraped_for_xlsx_log = [] # Log t·∫•t c·∫£ b√†i vi·∫øt ƒë√£ g·∫∑p
    processed_article_urls_for_content = set() # URLs ƒë√£ l·∫•y n·ªôi dung chi ti·∫øt

    try:
        gc = authenticate_google_sheets_service_account(SERVICE_ACCOUNT_JSON_PATH)
        if not gc:
            print("CRITICAL: Kh√¥ng th·ªÉ x√°c th·ª±c Google Sheets. D·ª´ng ch∆∞∆°ng tr√¨nh.")
            return

        try:
            driver = setup_selenium_driver()
        except Exception as driver_err:
            print(f"CRITICAL: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Selenium WebDriver: {driver_err}. D·ª´ng ch∆∞∆°ng tr√¨nh.")
            return

        gemini_model = setup_gemini(GEMINI_API_KEY)
        if not gemini_model:
            print("WARNING: Kh√¥ng th·ªÉ kh·ªüi t·∫°o Gemini Model. Ph√¢n t√≠ch AI s·∫Ω b·ªã b·ªè qua ho·∫∑c l·ªói.")
            # Kh√¥ng d·ª´ng h·∫≥n, nh∆∞ng c√°c tr∆∞·ªùng AI s·∫Ω b√°o l·ªói

        all_program_databases = read_all_competitor_program_dbs(gc, GOOGLE_SHEET_ID, COMPETITOR_PROGRAM_DB_SHEET_MAP)

        # --- Crawling Loop ---
        print(f"\nTruy c·∫≠p trang b·∫Øt ƒë·∫ßu: {VIETSTOCK_START_URL}")
        driver.get(VIETSTOCK_START_URL)
        current_page = 1
        stop_crawling = False

        while current_page <= MAX_PAGES_TO_CRAWL and not stop_crawling:
            print(f"\n--- Processing Page {current_page}/{MAX_PAGES_TO_CRAWL} ---")
            page_start_time = time.time()
            old_article_found_on_page = False

            try:
                wait = WebDriverWait(driver, SELENIUM_WAIT)
                # Ch·ªù container ch√≠nh c·ªßa danh s√°ch b√†i vi·∫øt
                container_selectors = ["#channel-container", "#category_content", "div.listing-news", "ul.list-news-font-size-medium"]
                container = None
                for sel in container_selectors:
                    try:
                        container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
                        if container: break
                    except TimeoutException:
                        continue
                if not container:
                    print(f"   Warning: Timed out waiting for any article container on page {current_page}.")
                    if current_page == 1 and ("<body" not in driver.page_source or len(driver.page_source) < 1000):
                        print("   Page 1 seems empty or failed to load. Stopping.")
                        break # D·ª´ng n·∫øu trang ƒë·∫ßu l·ªói n·∫∑ng
                    # C√≥ th·ªÉ th·ª≠ refresh ho·∫∑c b·ªè qua trang n√†y
                    current_page +=1 # Th·ª≠ sang trang ti·∫øp
                    time.sleep(SLEEP_BETWEEN_PAGES)
                    try: # Th·ª≠ click next page n·∫øu c√≥
                        next_button_sel = driver.find_element(By.CSS_SELECTOR, '.btn-page-next, a[title="Trang sau"], a.next')
                        if next_button_sel.is_displayed() and next_button_sel.is_enabled():
                            driver.execute_script("arguments[0].click();", next_button_sel)
                            time.sleep(SLEEP_BETWEEN_PAGES)
                            continue
                        else: break
                    except: break


                time.sleep(2) # Ch·ªù th√™m ch√∫t cho JS load (n·∫øu c√≥)
            except Exception as wait_err:
                print(f"   Error waiting for page elements: {wait_err}")
                break # D·ª´ng n·∫øu kh√¥ng t√¨m th·∫•y container

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            # ƒêi·ªÅu ch·ªânh selector cho c√°c b√†i vi·∫øt d·ª±a tr√™n c·∫•u tr√∫c th·ª±c t·∫ø c·ªßa Vietstock
            # V√≠ d·ª•: c√°c th·∫ª div ho·∫∑c li ch·ª©a th√¥ng tin b√†i vi·∫øt
            article_elements_soup = soup.select("div.single_post_text")
            if not article_elements_soup: # Fallback selectors
                article_elements_soup = soup.select("div.mb-3 > h3 > a") # T√¨m c√°c th·∫ª a l√† ti√™u ƒë·ªÅ
                article_elements_soup = [a.find_parent('div', class_=re.compile(r'item|article')) or a.find_parent('li') or a for a in article_elements_soup]


            print(f"   Found {len(article_elements_soup)} potential articles on page {current_page}.")
            if not article_elements_soup and current_page == 1:
                print("   No articles found on the first page. Check selectors or website structure.")
                break

            for article_soup_item in article_elements_soup: # article_soup_item gi·ªù l√† div.single_post_text (ho·∫∑c kh·ªëi cha n·∫øu d√πng C√°ch 2)
                title = "N/A"; article_url = None; article_time_list_utc = None
                try:
                    # L·∫•y link v√† title
                    # N·∫øu article_soup_item l√† div.single_post_text:
                    title_link_element = article_soup_item.select_one("h4 > a.fontbold")
                    if not title_link_element:
                         title_link_element = article_soup_item.select_one("h4 > a") # Fallback

                    # N·∫øu article_soup_item l√† kh·ªëi cha (v√≠ d·ª• div.article-wrapper), th√¨ ph·∫£i select s√¢u h∆°n:
                    # title_link_element = article_soup_item.select_one("div.single_post_text > h4 > a.fontbold")

                    if not title_link_element:
                        # print("    Skipping item, title_link_element not found.") # Debug
                        continue

                    title = title_link_element.get_text(strip=True) or title_link_element.get('title', 'N/A').strip()
                    raw_url = title_link_element.get('href')
                    if not raw_url:
                        # print(f"    Skipping item '{title[:30]}...', raw_url not found.") # Debug
                        continue

                    # ... (x·ª≠ l√Ω raw_url th√†nh article_url nh∆∞ c≈©) ...
                    if raw_url.startswith('http'): article_url = raw_url
                    elif raw_url.startswith('/'): article_url = BASE_URL + raw_url # Gi·∫£ s·ª≠ BASE_URL l√† "https://vietstock.vn" ho·∫∑c "http://fili.vn"
                    else: article_url = BASE_URL + '/' + raw_url.lstrip('/')
                    article_url = article_url.strip().split("?")[0]

                    # L·∫•y th·ªùi gian t·ª´ trang list
                    # N·∫øu article_soup_item l√† div.single_post_text:
                    time_element = article_soup_item.select_one("div.meta3 > a:nth-of-type(2)")
                    # N·∫øu article_soup_item l√† kh·ªëi cha:
                    # time_element = article_soup_item.select_one("div.single_post_text > div.meta3 > a:nth-of-type(2)")

                    date_text_list = None
                    if time_element:
                        # Th·ª≠ l·∫•y text tr·ª±c ti·∫øp, v√¨ ::before c√≥ th·ªÉ kh√¥ng ƒë∆∞·ª£c get_text() l·∫•y
                        # C·∫ßn ki·ªÉm tra xem text c·ªßa th·∫ª <a> n√†y c√≥ ch·ª©a ng√†y gi·ªù kh√¥ng, hay n√≥ n·∫±m ·ªü ƒë√¢u ƒë√≥ kh√°c
                        # N·∫øu text n·∫±m trong ::before, BeautifulSoup kh√¥ng l·∫•y ƒë∆∞·ª£c tr·ª±c ti·∫øp.
                        # B·∫°n c√≥ th·ªÉ c·∫ßn d√πng Selenium ƒë·ªÉ l·∫•y computed style ho·∫∑c text content bao g·ªìm pseudo-elements,
                        # ho·∫∑c t√¨m m·ªôt ngu·ªìn th·ªùi gian kh√°c tr√™n trang.
                        # T·∫°m th·ªùi, th·ª≠ get_text() v√† xem n√≥ tr·∫£ v·ªÅ g√¨.
                        date_text_list = time_element.get_text(strip=True)
                        # print(f"    Raw time text from list: '{date_text_list}' for article '{title[:30]}'") # Debug
                        if date_text_list: # C·∫ßn x·ª≠ l√Ω chu·ªói n√†y n·∫øu n√≥ kh√¥ng ph·∫£i ƒë·ªãnh d·∫°ng chu·∫©n
                            # V√≠ d·ª•, n·∫øu n√≥ l√† "QUY HO·∫†CH - H·∫† T·∫¶NG | 28/05 16:56"
                            # C·∫ßn tr√≠ch xu·∫•t "28/05 16:56"
                            match_time = re.search(r'(\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2})', date_text_list)
                            if match_time:
                                date_text_list = match_time.group(1)
                                # print(f"    Extracted time text: '{date_text_list}'") # Debug
                                article_time_list_utc = parse_article_time(date_text_list, start_crawl_time_utc)
                            else: # N·∫øu kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c, th·ª≠ t√¨m trong title c·ªßa th·∫ª a
                                time_title_attr = time_element.get('title')
                                if time_title_attr:
                                     match_time_title = re.search(r'(\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{1,2}:\d{1,2})|(\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2})', time_title_attr)
                                     if match_time_title:
                                         date_text_list = match_time_title.group(0) # L·∫•y to√†n b·ªô match
                                         article_time_list_utc = parse_article_time(date_text_list, start_crawl_time_utc)

                    # Ki·ªÉm tra n·∫øu b√†i qu√° c≈© (d·ª±a tr√™n th·ªùi gian t·ª´ trang list)
                    if article_time_list_utc and article_time_list_utc < time_limit_utc:
                        print(f"      Article '{title[:40]}...' (list time) too old. Stopping page processing.")
                        old_article_found_on_page = True; break # D·ª´ng x·ª≠ l√Ω c√°c b√†i c√≤n l·∫°i tr√™n trang n√†y

                    # Log t·∫•t c·∫£ b√†i vi·∫øt ƒë√£ g·∫∑p (cho file XLSX)
                    temp_article_time_str = article_time_list_utc.astimezone(pytz.timezone(TARGET_TIMEZONE)).strftime('%Y-%m-%d %H:%M:%S') if article_time_list_utc else "N/A"
                    all_articles_scraped_for_xlsx_log.append({
                        'URL': article_url, 'Ti√™u ƒë·ªÅ': title,
                        'Th·ªùi gian (List View)': temp_article_time_str, 'Ngu·ªìn': 'Vietstock'
                    })

                    # --- X·ª≠ l√Ω s√¢u n·∫øu URL ch∆∞a ƒë∆∞·ª£c l·∫•y n·ªôi dung ---
                    if article_url not in processed_article_urls_for_content:
                        processed_article_urls_for_content.add(article_url) # ƒê√°nh d·∫•u ƒë√£ x·ª≠ l√Ω URL n√†y

                        print(f"\n  Fetching details for: {title[:60]}... ({article_url})")
                        full_content, _, article_time_detail_utc = fetch_article_details(article_url, start_crawl_time_utc)

                        # ∆Øu ti√™n th·ªùi gian t·ª´ trang chi ti·∫øt, n·∫øu kh√¥ng c√≥ th√¨ d√πng th·ªùi gian t·ª´ trang list
                        final_article_time_utc = article_time_detail_utc or article_time_list_utc
                        if final_article_time_utc and final_article_time_utc < time_limit_utc:
                            print(f"    Article '{title[:40]}...' (detail/final time) too old. Skipping further analysis.")
                            continue # B·ªè qua b√†i n√†y, nh∆∞ng kh√¥ng d·ª´ng c·∫£ trang (c√≥ th·ªÉ b√†i sau m·ªõi h∆°n)

                        if isinstance(full_content, str) and "L·ªói" not in full_content and len(full_content) > 100:
                            # T√¨m keywords trong title v√† content
                            title_lower = title.lower()
                            content_lower = full_content.lower() # D√πng full_content ƒë√£ l·∫•y
                            matched_competitor_keys, all_kws_found = find_keywords_and_competitors(content_lower, title_lower)

                            if matched_competitor_keys:
                                print(f"    Keywords found for: {', '.join(sorted(list(matched_competitor_keys)))} in '{title[:50]}...'")
                                # V·ªõi m·ªói c√¥ng ty ƒë·ªëi th·ªß t√¨m th·∫•y trong b√†i, ch·∫°y ph√¢n t√≠ch
                                for comp_key in matched_competitor_keys:
                                    if not gemini_model:
                                        print(f"    Skipping AI for {comp_key} due to Gemini model init failure.")
                                        # T·∫°o m·ªôt b·∫£n ghi l·ªói c∆° b·∫£n
                                        error_output_row = {h: None for h in SHEET_HEADER_OUTPUT}
                                        error_output_row.update({
                                            'Th·ªùi gian c·∫≠p nh·∫≠t': crawl_time_str_local, 'Ti√™u ƒë·ªÅ b√†i vi·∫øt': title,
                                            'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch': comp_key, 'URL': article_url,
                                            'L√† c√¥ng ty ƒë·ªëi th·ªß?': True, 'Tr·∫°ng th√°i x·ª≠ l√Ω': "L·ªói Model AI",
                                            'N·ªôi dung g·ªëc': full_content[:MAX_CONTENT_LENGTH_FOR_AI]
                                        })
                                        all_output_data_for_sheet.append(error_output_row)
                                        continue

                                    # G·ªçi h√†m x·ª≠ l√Ω ch√≠nh cho t·ª´ng ƒë·ªëi th·ªß
                                    output_data_row = process_single_article_for_competitor(
                                        title, article_url, full_content,
                                        comp_key, all_kws_found, # Truy·ªÅn list keyword t√¨m th·∫•y
                                        crawl_time_str_local,
                                        gemini_model, all_program_databases
                                    )
                                    if output_data_row:
                                        output_data_row['L√† c√¥ng ty ƒë·ªëi th·ªß?'] = True # V√¨ comp_key n·∫±m trong matched_competitor_keys
                                        all_output_data_for_sheet.append(output_data_row)
                            else:
                                print(f"    No relevant keywords in '{title[:50]}...'. Skipping AI analysis for this article.")
                                # V·∫´n c√≥ th·ªÉ ghi 1 d√≤ng "Kh√¥ng c√≥ ƒë·ªëi th·ªß" n·∫øu mu·ªën
                                no_competitor_row = {h: None for h in SHEET_HEADER_OUTPUT}
                                no_competitor_row.update({
                                    'Th·ªùi gian c·∫≠p nh·∫≠t': crawl_time_str_local, 'Ti√™u ƒë·ªÅ b√†i vi·∫øt': title,
                                    'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch': "Kh√¥ng c√≥", 'URL': article_url,
                                    'L√† c√¥ng ty ƒë·ªëi th·ªß?': False, 'Tr·∫°ng th√°i x·ª≠ l√Ω': "Kh√¥ng c√≥ t·ª´ kh√≥a ƒë·ªëi th·ªß",
                                    'N·ªôi dung g·ªëc': full_content[:MAX_CONTENT_LENGTH_FOR_AI]
                                })
                                all_output_data_for_sheet.append(no_competitor_row)
                        else:
                            print(f"    Content error or insufficient for '{title[:50]}...'. Skipping analysis.")
                            error_content_row = {h: None for h in SHEET_HEADER_OUTPUT}
                            error_content_row.update({
                                'Th·ªùi gian c·∫≠p nh·∫≠t': crawl_time_str_local, 'Ti√™u ƒë·ªÅ b√†i vi·∫øt': title,
                                'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch': "N/A", 'URL': article_url,
                                'L√† c√¥ng ty ƒë·ªëi th·ªß?': False, # Kh√¥ng bi·∫øt
                                'Tr·∫°ng th√°i x·ª≠ l√Ω': "L·ªói t·∫£i/x·ª≠ l√Ω n·ªôi dung",
                                'N·ªôi dung g·ªëc': str(full_content)[:200] # Ghi l·∫°i l·ªói
                            })
                            all_output_data_for_sheet.append(error_content_row)
                    # else:
                    #     print(f"  Content for {article_url} already processed. Skipping.")

                except KeyboardInterrupt: raise
                except Exception as article_loop_err:
                    print(f"   L·ªñI khi x·ª≠ l√Ω m·ªôt b√†i vi·∫øt '{title[:50]}...': {article_loop_err}")
                    traceback.print_exc(limit=1)
                    # Ghi nh·∫≠n l·ªói n√†y v√†o sheet
                    error_loop_row = {h: None for h in SHEET_HEADER_OUTPUT}
                    error_loop_row.update({
                        'Th·ªùi gian c·∫≠p nh·∫≠t': crawl_time_str_local, 'Ti√™u ƒë·ªÅ b√†i vi·∫øt': title or "L·ªói l·∫•y ti√™u ƒë·ªÅ",
                        'C√¥ng ty ƒë∆∞·ª£c ph√¢n t√≠ch': "N/A", 'URL': article_url or "L·ªói l·∫•y URL",
                        'L√† c√¥ng ty ƒë·ªëi th·ªß?': False,
                        'Tr·∫°ng th√°i x·ª≠ l√Ω': f"L·ªói v√≤ng l·∫∑p x·ª≠ l√Ω: {str(article_loop_err)[:100]}",
                        'N·ªôi dung g·ªëc': "L·ªói"
                    })
                    all_output_data_for_sheet.append(error_loop_row)


            if old_article_found_on_page:
                print("   Old article detected on page, stopping crawl for subsequent pages.")
                stop_crawling = True; break

            print(f"   Finished page {current_page}. Processing time: {time.time() - page_start_time:.2f}s.")
            if current_page >= MAX_PAGES_TO_CRAWL:
                print(f"   Reached max pages ({MAX_PAGES_TO_CRAWL}). Stopping."); break

            # --- Pagination ---
            current_page += 1
            print(f"   Attempting to navigate to page {current_page}...")
            try:
                # Th·ª≠ c√°c selectors ph·ªï bi·∫øn cho n√∫t "Next" ho·∫∑c s·ªë trang ti·∫øp theo
                next_button_selectors = [
                    f"//a[contains(@href, 'page={current_page}') and not(contains(@class, 'current'))]", # XPath cho link c√≥ page=
                    f"//a[normalize-space()='{current_page}']", # XPath cho s·ªë trang
                    "a.next", "a.next-page", "a[rel='next']",
                    "a[title*='Trang sau'], a[title*='Next']",
                    ".pagination li.active + li a", # Link trong li k·∫ø ti·∫øp li active
                    "//a[contains(text(),'Sau') or contains(text(),'Ti·∫øp') or contains(text(),'>')]"
                ]
                next_button = None
                for sel_type, sel_val in [("xpath", s) if s.startswith("//") else ("css",s) for s in next_button_selectors]:
                    try:
                        if sel_type == "xpath":
                            next_button = driver.find_element(By.XPATH, sel_val)
                        else: # css
                            next_button = driver.find_element(By.CSS_SELECTOR, sel_val)

                        if next_button and next_button.is_displayed() and next_button.is_enabled():
                            print(f"   Found next page element with selector: {sel_val}")
                            break
                        else: next_button = None
                    except NoSuchElementException:
                        next_button = None
                if next_button:
                    driver.execute_script("arguments[0].scrollIntoViewIfNeeded(true);", next_button)
                    time.sleep(0.5)
                    driver.execute_script("arguments[0].click();", next_button)
                    print(f"   Clicked 'Next' or page {current_page}. Waiting for page load...")
                    time.sleep(SLEEP_BETWEEN_PAGES)
                else:
                    print("   Could not find a 'Next' button or link for the next page. Stopping pagination.")
                    stop_crawling = True
            except Exception as page_err:
                print(f"   Error during pagination to page {current_page}: {page_err}")
                stop_crawling = True

    except KeyboardInterrupt:
        print("\nKeyboardInterrupt detected. Stopping crawl...")
    except Exception as e:
        print(f"\n--- UNEXPECTED ERROR DURING CRAWL ---")
        print(f"Error: {e}")
        traceback.print_exc()
    finally:
        if driver:
            print("Closing Selenium WebDriver...")
            driver.quit()

    # --- Save Results ---
    sheet_write_successful = False
    if gc and all_output_data_for_sheet:
        print(f"\nAttempting to write {len(all_output_data_for_sheet)} processed rows to Google Sheet '{TARGET_WORKSHEET_NAME}'...")
        sheet_write_successful = write_data_to_google_sheet(
            gc, GOOGLE_SHEET_ID, TARGET_WORKSHEET_NAME,
            SHEET_HEADER_OUTPUT, # S·ª≠ d·ª•ng header output ƒë√£ ƒë·ªãnh nghƒ©a
            all_output_data_for_sheet
        )
    elif not all_output_data_for_sheet:
        print("\nNo data processed to write to Google Sheet.")
    else:
        print("\nGoogle Sheets client not available or failed, cannot write to Sheet.")

    # --- Save XLSX Log (Th√¥ng tin c∆° b·∫£n c·ªßa t·∫•t c·∫£ b√†i ƒë√£ g·∫∑p) ---
    if all_articles_scraped_for_xlsx_log:
        print(f"\nAttempting to save {len(all_articles_scraped_for_xlsx_log)} scraped article (log) details to XLSX...")
        xlsx_filename = f"ALL_vietstock_articles_LOG_{start_crawl_time_local.strftime('%Y%m%d_%H%M%S')}.xlsx"
        try:
            df_all_log = pd.DataFrame(all_articles_scraped_for_xlsx_log)
            df_all_log.to_excel(xlsx_filename, index=False, engine='openpyxl')
            print(f"‚úÖ ***** ALL SCRAPED ARTICLE (LOG) INFO SAVED TO: {xlsx_filename} *****")
            try:
                from google.colab import files
                files.download(xlsx_filename)
            except Exception as download_err:
                print(f"   L·ªñI: Could not automatically download '{xlsx_filename}': {download_err}")
        except ImportError:
            print("WARNING: `openpyxl` not found. Cannot save XLSX log. Run `pip install openpyxl`")
        except Exception as xlsx_err:
            print(f"L·ªñI: Failed to save XLSX log file '{xlsx_filename}': {xlsx_err}")
    else:
        print("\nNo articles were logged to save to XLSX.")

    # --- Save CSV backup for the MAIN output sheet data IF Sheets failed ---
    if not sheet_write_successful and all_output_data_for_sheet:
        print(f"\nGoogle Sheet write failed or skipped. Saving main output data to local CSV backup...")
        try:
            df_backup = pd.DataFrame(all_output_data_for_sheet)
            # ƒê·∫£m b·∫£o c√°c c·ªôt theo ƒë√∫ng th·ª© t·ª± c·ªßa SHEET_HEADER_OUTPUT
            df_backup = df_backup.reindex(columns=SHEET_HEADER_OUTPUT, fill_value='')
            backup_filename = f"vietstock_quangcao_BACKUP_{start_crawl_time_local.strftime('%Y%m%d_%H%M%S')}.csv"
            df_backup.to_csv(backup_filename, index=False, encoding='utf-8-sig')
            print(f"‚ÑπÔ∏è ***** Main output data SAVED TO LOCAL BACKUP CSV: {backup_filename} *****")
        except Exception as csv_err:
            print(f"L·ªñI: Failed to save backup CSV: {csv_err}")

    end_crawl_time_local = get_vietnam_time()
    print(f"\n[{end_crawl_time_local.strftime('%Y-%m-%d %H:%M:%S %Z')}] Vietstock crawl finished.")
    total_duration = end_crawl_time_local - start_crawl_time_local
    print(f"Total duration: {total_duration}")
    print(f"Total articles logged for XLSX: {len(all_articles_scraped_for_xlsx_log)}")
    print(f"Total rows prepared for Google Sheet '{TARGET_WORKSHEET_NAME}': {len(all_output_data_for_sheet)}")

print("Core processing logic defined.")

# Cell 5: Run Main Function
if __name__ == "__main__":
    # Make sure pandas is available for XLSX and CSV writing
    try:
        import pandas as pd
        import openpyxl # Needed for df.to_excel
    except ImportError as import_err:
        print(f"Import Error: {import_err}")
        print("Please ensure pandas and openpyxl are installed (`pip install pandas openpyxl`)")
        raise

    crawl_vietstock()