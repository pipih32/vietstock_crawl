# -*- coding: utf-8 -*-
"""vietstock _ crawl (demo)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YEExJlNlMRo60tUlNabwu06GZfA873gx
"""

from google.colab import drive
drive.mount('/content/drive')

print("Google Drive mounted successfully at /content/drive")

# Cell 1: Installations and Imports
# Install necessary packages (run once)
!pip install requests beautifulsoup4 pandas selenium google-generativeai google-auth gspread pytz fuzzywuzzy python-Levenshtein google-auth-oauthlib google-auth-httplib2 -q

# --- Colab Specific Setup for Chrome/ChromeDriver ---
# (This block MUST be run successfully)
print("Updating apt and installing prerequisites...")
!sudo apt-get update >> /dev/null
!sudo apt-get install -y wget gnupg unzip >> /dev/null
print("Downloading and installing Google Chrome...")
!wget -q -O google-chrome-stable_current_amd64.deb https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!sudo dpkg -i google-chrome-stable_current_amd64.deb >> /dev/null
!sudo apt-get install -f -y >> /dev/null # Install dependencies
!rm google-chrome-stable_current_amd64.deb # Clean up
print("Downloading and installing ChromeDriver...")

import subprocess
import os

try:
    chrome_version_full_bytes = subprocess.check_output(['google-chrome', '--version'])
    chrome_version_full = chrome_version_full_bytes.decode('utf-8').strip()
    try:
        chrome_version = chrome_version_full.split(' ')[-1].split('.')[0]
    except IndexError:
        print(f"ERROR: Could not parse Chrome version from: '{chrome_version_full}'")
        raise ValueError("Failed to parse Chrome version")
    print(f"Detected Chrome version: {chrome_version_full} (Major: {chrome_version})")

    wget_command = f'wget -qO- "https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_{chrome_version}"'
    chromedriver_version_bytes = subprocess.check_output(wget_command, shell=True, stderr=subprocess.PIPE)
    chromedriver_version = chromedriver_version_bytes.decode('utf-8').strip()
    if not chromedriver_version or "Error" in chromedriver_version:
         raise ValueError(f"Could not find matching ChromeDriver version for Chrome {chrome_version}. Wget output: {chromedriver_version}")
    print(f"Using ChromeDriver version: {chromedriver_version}")

    print("Downloading ChromeDriver zip...")
    !wget -q -O chromedriver-linux64.zip "https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/$chromedriver_version/linux64/chromedriver-linux64.zip"
    print("Unzipping ChromeDriver...")
    !unzip -q chromedriver-linux64.zip -d /usr/local/bin/

    chromedriver_unzipped_path = "/usr/local/bin/chromedriver-linux64/chromedriver"
    chromedriver_direct_path = "/usr/local/bin/chromedriver"

    if os.path.exists(chromedriver_unzipped_path):
        print(f"Moving {chromedriver_unzipped_path} to {chromedriver_direct_path}...")
        !sudo mv {chromedriver_unzipped_path} {chromedriver_direct_path}
        print("Cleaning up extracted directory...")
        !rm -r /usr/local/bin/chromedriver-linux64
    elif os.path.exists(chromedriver_direct_path):
        print("ChromeDriver found directly in /usr/local/bin/.")
    else:
        # Try current directory fallback
        if os.path.exists("./chromedriver-linux64/chromedriver"):
             print("Found chromedriver in current directory structure, attempting move...")
             !sudo mv ./chromedriver-linux64/chromedriver {chromedriver_direct_path}
             !rm -r ./chromedriver-linux64
        elif os.path.exists("./chromedriver"):
             print("Found chromedriver directly in current directory, attempting move...")
             !sudo mv ./chromedriver {chromedriver_direct_path}
        else:
            raise FileNotFoundError("Could not locate chromedriver executable after unzip.")

    print("Setting execution permissions...")
    !sudo chmod +x /usr/local/bin/chromedriver
    print("Cleaning up zip file...")
    !rm chromedriver-linux64.zip

    if os.path.exists(chromedriver_direct_path):
        print(f"ChromeDriver successfully installed to: {chromedriver_direct_path}")
    else:
        raise FileNotFoundError("ChromeDriver final installation check failed.")

except Exception as e:
    print(f"ERROR during automatic ChromeDriver download/setup: {e}")
    print("Falling back to installing a fixed ChromeDriver version via apt...")
    !apt-get install -y chromium-chromedriver

print("Browser and Driver installation attempt complete.")
# ----------------------------------------------------

# Python Imports (after installations)
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import re
import json
import pytz
import traceback
import gspread
from google.colab import auth # For user authentication
from google.auth import default # To get default credentials after auth
from google.auth import exceptions as google_auth_exceptions
import google.generativeai as genai

# --- Selenium Imports ---
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException

print("All libraries imported.")

# Cell 2: Configuration with Colab Forms

# --- General Configuration ---
VIETSTOCK_START_URL = "https://vietstock.vn/chung-khoan.htm"
BASE_URL = "https://vietstock.vn"
MAX_PAGES_TO_CRAWL = 5 #@param {type:"integer"} # Giảm để test nhanh
HOURS_TO_CHECK = 72 #@param {type:"integer"} # Default to 3 days
PROGRAM_SIMILARITY_THRESHOLD = 85 #@param {type:"integer"} # Fuzzy match threshold for programs (0-100) - Sẽ dùng lại

# --- Keyword Configuration ---
# KEYWORDS sẽ bao gồm cả tên công ty và tên các lãnh đạo chủ chốt nếu muốn focus vào bài phỏng vấn CEO
KEYWORDS = {
    # Competitor Key: [List of keyword variations, including CEO names if desired]
    your_choice
    # Thêm các công ty khác nếu cần
}

# --- Google Sheet Configuration ---
GOOGLE_SHEET_ID = "your_choice" #@param {type:"string"}
TARGET_WORKSHEET_NAME = "your_choice" #@param {type:"string"}

# Map Competitor Key to the Google Sheet Tab name containing ITS OWN program list
# Dùng để đọc danh sách chương trình hiện có của từng công ty
COMPETITOR_PROGRAM_DB_SHEET_MAP = {
    your_choice
    # Thêm các công ty khác nếu có sheet chương trình tương ứng
}

# --- NEW: Define the exact header order for the target sheet (vietstock_quảng_cáo) ---
SHEET_HEADER_OUTPUT = [
    your_choice]

# --- Gemini AI Configuration ---
GEMINI_API_KEY = "your_choice" #@param {type:"string"} # IMPORTANT: Replace with your actual key or use Colab Secrets

if not GEMINI_API_KEY or "YOUR_GEMINI_API_KEY" in GEMINI_API_KEY or len(GEMINI_API_KEY) < 15:
     print("⚠️ WARNING: Please set a valid Gemini API Key in the form above.")
     # raise ValueError("LỖI: Vui lòng đặt Gemini API Key hợp lệ vào form cấu hình.")

GEMINI_MODEL_NAME = 'gemini-1.5-flash-latest' # Hoặc 'gemini-pro'

# --- Timezone Configuration ---
TARGET_TIMEZONE = 'Asia/Ho_Chi_Minh' # GMT+7

# --- Service Account Configuration for Google Sheets ---
# IMPORTANT: Upload your service account JSON key file to Colab or Google Drive
# and provide the correct path here.
_SERVICE_ACCOUNT_JSON_PATH_FORM = "your_choice" #@param {type:"string"}
SERVICE_ACCOUNT_JSON_PATH = None
if _SERVICE_ACCOUNT_JSON_PATH_FORM:
    SERVICE_ACCOUNT_JSON_PATH = _SERVICE_ACCOUNT_JSON_PATH_FORM
else:
    print("⚠️ WARNING: Service Account JSON path is not set. Google Sheets access will fail.")
    # raise ValueError("LỖI: Vui lòng cung cấp đường dẫn đến file JSON của Service Account.")

# --- Other Constants ---
REQUESTS_TIMEOUT = 20
SELENIUM_WAIT = 15
SLEEP_BETWEEN_PAGES = 3
SLEEP_BETWEEN_AI_CALLS = 2 # Giữ khoảng nghỉ quan trọng
SHEET_WRITE_BATCH_SIZE = 50
SHEET_WRITE_DELAY = 1
MAX_CONTENT_LENGTH_FOR_AI = 15000 # Giới hạn ký tự gửi cho AI để tránh quá dài

print("Configuration loaded.")
print(f"Target Sheet: {TARGET_WORKSHEET_NAME} in Sheet ID: {GOOGLE_SHEET_ID}")
print(f"Output Sheet Header: {SHEET_HEADER_OUTPUT}")
if SERVICE_ACCOUNT_JSON_PATH:
    print(f"Using Service Account Key: {SERVICE_ACCOUNT_JSON_PATH}")

# Cell 3: Helper Functions

import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import re
import json
import pytz
import traceback
import gspread # Sẽ dùng gspread.service_account
# from google.colab import auth # Không dùng user auth nữa
# from google.auth import default # Không dùng default creds nữa
from google.oauth2.service_account import Credentials # Dùng cho Service Account
import google.generativeai as genai
from fuzzywuzzy import fuzz, process as fuzzy_process # Thêm process

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException
import os

# --- Time Function ---
def get_vietnam_time():
    return datetime.now(pytz.timezone(TARGET_TIMEZONE))

# --- Google Sheets Authentication with Service Account ---
def authenticate_google_sheets_service_account(json_keyfile_path: str) -> gspread.Client | None:
    """Authenticates using a Service Account JSON key file."""
    if not json_keyfile_path:
        print("❌ Critical Error: Service Account JSON path is missing.")
        return None
    if not os.path.exists(json_keyfile_path):
        print(f"❌ Critical Error: Service Account JSON file not found at: {json_keyfile_path}")
        print("   Please upload the file and provide the correct path in Cell 2.")
        return None
    try:
        print(f"🔑 Authenticating Google Sheets using Service Account: {json_keyfile_path}...")
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive' # Có thể cần quyền drive nếu tạo sheet mới
        ]
        creds = Credentials.from_service_account_file(json_keyfile_path, scopes=scopes)
        gc = gspread.authorize(creds)
        print("✅ Google Sheets authentication with Service Account successful.")
        # Test connection by listing spreadsheets (optional)
        # gc.list_spreadsheet_files()
        return gc
    except Exception as e:
        print(f"❌ Critical Error: Failed Google Sheets Service Account authentication: {e}")
        if "invalid_grant" in str(e).lower() or "permission denied" in str(e).lower():
             print("   Hint: Check if the Service Account has 'Editor' access to the Google Sheet.")
             print("   Also, ensure the 'Google Sheets API' and 'Google Drive API' are enabled in your GCP project.")
        traceback.print_exc()
        return None

# --- Selenium Setup (Giữ nguyên hoặc điều chỉnh nếu cần) ---
def setup_selenium_driver():
    # ... (Code setup_selenium_driver giữ nguyên như phiên bản gốc của bạn) ...
    print("Setting up Selenium WebDriver...")
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--disable-extensions")
    options.add_argument("--proxy-server='direct://'")
    options.add_argument("--proxy-bypass-list=*")
    options.add_argument("--start-maximized")

    chromedriver_path = "/usr/local/bin/chromedriver"
    if not os.path.exists(chromedriver_path):
        chromedriver_path = "/usr/bin/chromedriver"

    if not os.path.exists(chromedriver_path):
         print(f"ERROR: ChromeDriver executable not found at default paths.")
         raise FileNotFoundError("ChromeDriver executable not found.")

    print(f"Using ChromeDriver at: {chromedriver_path}")
    try:
        service = Service(executable_path=chromedriver_path)
        driver = webdriver.Chrome(service=service, options=options)
        print("WebDriver setup successful.")
        return driver
    except WebDriverException as e:
        print(f"WebDriver setup failed: {e}")
        raise
    except Exception as general_err:
         print(f"An unexpected error occurred during WebDriver setup: {general_err}")
         raise

# --- Read Program Databases from Sheets ---
def read_all_competitor_program_dbs(gc: gspread.Client, sheet_id: str, db_map: dict) -> dict:
    """
    Reads program names and details from ALL specified competitor program sheets.
    Returns a dict: { "CompetitorKey": [{"name": "Prog Name", "details": "Details"}, ...], ... }
    """
    print("Reading all competitor program databases from Google Sheets...")
    all_programs_data = {}
    if not gc:
        print("Warning: Google Sheets client not available. Cannot read program databases.")
        return all_programs_data

    try:
        spreadsheet = gc.open_by_key(sheet_id)
    except Exception as e:
        print(f"LỖI: Cannot access Google Sheet (ID: {sheet_id}) to read program DBs: {e}")
        return all_programs_data

    for competitor_key, sheet_name in db_map.items():
        all_programs_data[competitor_key] = []
        try:
            worksheet = spreadsheet.worksheet(sheet_name)
            all_values = worksheet.get_all_values()
            if not all_values or len(all_values) < 2:
                print(f"   Warning: Program DB sheet '{sheet_name}' for {competitor_key} is empty.")
                continue

            header_row = all_values[0]
            data_rows = all_values[1:]
            # Giả sử cột 'Tên' là cột A (index 0) và 'Đặc điểm' là cột B (index 1)
            # Hoặc bạn có thể tìm index động như trước
            try: name_col_idx = header_row.index("Tên") # Hoặc tên cột chứa tên chương trình
            except ValueError: name_col_idx = 0; print(f"   Warning: 'Tên' header not found in '{sheet_name}', using column A.")
            try: details_col_idx = header_row.index("Đặc điểm") # Hoặc tên cột chứa mô tả
            except ValueError: details_col_idx = 1; print(f"   Warning: 'Đặc điểm' header not found in '{sheet_name}', using column B.")

            print(f"   Reading program DB for {competitor_key} from sheet '{sheet_name}'...")
            for row in data_rows:
                name = str(row[name_col_idx]).strip() if len(row) > name_col_idx and row[name_col_idx] else ""
                details = str(row[details_col_idx]).strip() if len(row) > details_col_idx and row[details_col_idx] else ""
                if name:
                    all_programs_data[competitor_key].append({"name": name, "details": details})
            print(f"   -> Stored {len(all_programs_data[competitor_key])} programs for {competitor_key}.")
        except gspread.exceptions.WorksheetNotFound:
            print(f"   Warning: Program DB sheet '{sheet_name}' not found for {competitor_key}. Skipping.")
        except Exception as e:
            print(f"   Error reading program DB sheet '{sheet_name}' for {competitor_key}: {e}")
    print("Finished reading all competitor program databases.")
    return all_programs_data

# --- Gemini AI Setup (Giữ nguyên) ---
def setup_gemini(api_key: str) -> genai.GenerativeModel | None:
    # ... (Code setup_gemini giữ nguyên như phiên bản gốc của bạn) ...
    print(f"Configuring Gemini AI with model '{GEMINI_MODEL_NAME}'...")
    if not api_key or "YOUR_API_KEY" in api_key or len(api_key) < 15:
         print("ERROR: Invalid or missing Gemini API Key provided in configuration.")
         return None
    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        print("Gemini AI configured successfully.")
        return model
    except Exception as e:
        print(f"LỖI: Error configuring Gemini AI: {e}")
    return None

# --- NEW: Gemini Analysis Function (AI Call 1) ---
def analyze_article_with_gemini_v2(
    model: genai.GenerativeModel,
    title: str,
    content: str, # Full content, will be truncated
    competitor_being_analyzed: str, # Tên công ty đang được tập trung phân tích
    all_keywords_found_in_article: list # List các keyword cụ thể tìm thấy trong bài
) -> dict:
    """
    Analyzes article content using Gemini AI.
    Focuses on the competitor_being_analyzed.
    """
    print(f"  [AI Analysis] Analyzing for '{competitor_being_analyzed}': '{title[:50]}...'")
    # Các trường output mong muốn từ AI
    default_result = {
        "is_advertising": "Không xác định", # PR Chương trình/PR Thương hiệu/Không Quảng Cáo
        "promoted_program_product_name_ai": None, # Tên SP/CT AI trích xuất
        "main_message_ai": "Lỗi AI",
        "analysis_summary_llm_ai": "Lỗi AI",
        "article_focus_ai": "Không xác định", # Sản phẩm/Thương hiệu/Thị trường/Lãnh đạo
        "detailed_classification_ai": "Không xác định",
        "tone_ai": "Không xác định", # Tích cực/Trung lập/Tiêu cực/Quảng bá
        "_error": True
    }
    if not model:
        return {**default_result, "analysis_summary_llm_ai": "Lỗi Model AI"}

    truncated_content = content[:MAX_CONTENT_LENGTH_FOR_AI]
    if not truncated_content or len(truncated_content) < 100: # Cần đủ nội dung
        return {**default_result, "analysis_summary_llm_ai": "Nội dung không đủ để phân tích"}

    # Nhấn mạnh việc phân tích kỹ nếu có keyword
    keyword_emphasis = ""
    if competitor_being_analyzed.lower() in [kw.lower() for kw in all_keywords_found_in_article]:
        keyword_emphasis = f"Bài viết này có chứa từ khóa trực tiếp liên quan đến '{competitor_being_analyzed}' (ví dụ: {', '.join(all_keywords_found_in_article)}), vì vậy hãy phân tích kỹ lưỡng khả năng đây là một bài PR cho công ty này hoặc sản phẩm/dịch vụ của họ, ngay cả khi chỉ được đề cập một cách tinh tế."
    elif any(ceo_kw.lower() in title.lower() or ceo_kw.lower() in truncated_content.lower() for ceo_kw in KEYWORDS.get(competitor_being_analyzed, [])):
        keyword_emphasis = f"Bài viết này có thể liên quan đến lãnh đạo của '{competitor_being_analyzed}'. Hãy xem xét khả năng đây là một bài PR thương hiệu thông qua phỏng vấn hoặc phát biểu của lãnh đạo."


    prompt = f"""
    Bạn là một chuyên viên phân tích truyền thông ngành tài chính - chứng khoán.
    Phân tích bài báo tiếng Việt sau đây, tập trung đánh giá về công ty "{competitor_being_analyzed}".

    Tiêu đề: {title}
    Nội dung (trích đoạn): {truncated_content}
    {keyword_emphasis}

    Dựa **CHỈ** vào nội dung bài báo được cung cấp, hãy trả lời các câu hỏi sau dưới dạng một đối tượng JSON:

    1.  `is_advertising`: (String) Phân loại bài viết này liên quan đến "{competitor_being_analyzed}" vào MỘT trong ba nhóm chính:
        *   "PR Chương trình/Sản phẩm": Nếu nội dung chính là quảng bá, giới thiệu một sản phẩm, dịch vụ, chương trình ưu đãi hoặc sự kiện cụ thể của "{competitor_being_analyzed}".
        *   "PR Thương hiệu": Nếu nội dung chính là xây dựng hình ảnh, uy tín, giới thiệu về công ty "{competitor_being_analyzed}", lãnh đạo, giải thưởng, hoạt động CSR, báo cáo thường niên, hoặc phân tích thị trường do công ty phát hành mà KHÔNG tập trung quảng bá một chương trình/sản phẩm cụ thể nào. Bao gồm cả các bài phỏng vấn CEO/lãnh đạo.
        *   "Không Quảng Cáo": Nếu nội dung là tin tức thị trường chung không do "{competitor_being_analyzed}" tự PR, phân tích vĩ mô, thông tin về các công ty khác, hoặc không nhằm mục đích quảng bá cho "{competitor_being_analyzed}".

    2.  `promoted_program_product_name_ai`: (String hoặc null) Nếu `is_advertising` là "PR Chương trình/Sản phẩm", hãy trích xuất tên cụ thể của chương trình/sản phẩm/dịch vụ được quảng bá. Nếu không, để là null. Cố gắng trích xuất tên đầy đủ và chính xác nhất có thể.

    3.  `main_message_ai`: (String) Thông điệp chính hoặc điểm cốt lõi của bài viết liên quan đến "{competitor_being_analyzed}" là gì? Tóm tắt rất ngắn gọn (1-2 câu).

    4.  `analysis_summary_llm_ai`: (String) Tóm tắt phân tích sâu hơn về mục đích hoặc hàm ý chính của bài viết đối với "{competitor_being_analyzed}" (ví dụ: "Bài viết nhằm thu hút khách hàng mới cho sản phẩm X", "Bài viết khẳng định vị thế dẫn đầu của công ty Y trên thị trường Z", "Bài viết cung cấp góc nhìn của chuyên gia từ công ty A về triển vọng ngành"). (2-3 câu)

    5.  `article_focus_ai`: (String) Trọng tâm chính của bài viết là gì? Chọn MỘT trong các giá trị: "Sản phẩm/Dịch vụ", "Thương hiệu công ty", "Lãnh đạo công ty", "Phân tích/Nhận định thị trường", "Tin tức chung", "Khác".

    6.  `detailed_classification_ai`: (String) Dựa vào các phân tích trên, hãy đưa ra một phân loại chi tiết hơn. Ví dụ: "PR Sản phẩm hiện có", "PR Sản phẩm mới tiềm năng", "PR Thương hiệu - Phỏng vấn CEO", "PR Thương hiệu - Giải thưởng", "PR Thương hiệu - CSR", "Nhận định thị trường từ chuyên gia công ty", "Tin tức hoạt động công ty", "Không liên quan".

    7.  `tone_ai`: (String) Giọng điệu chung của bài viết khi nói về "{competitor_being_analyzed}" hoặc sản phẩm/chủ đề liên quan là gì? Chọn MỘT: "Tích cực", "Tiêu cực", "Trung lập", "Quảng bá/Ca ngợi", "Cảnh báo/Chê bai".

    **Yêu cầu định dạng:** Trả về một đối tượng JSON **DUY NHẤT** và **HỢP LỆ**.
    **KHÔNG** bao gồm bất kỳ giải thích nào bên ngoài cấu trúc JSON. **KHÔNG** sử dụng markdown code blocks.

    Ví dụ JSON (nếu PR sản phẩm):
    {{
        "is_advertising": "PR Chương trình/Sản phẩm",
        "promoted_program_product_name_ai": "Gói vay Margin Super X",
        "main_message_ai": "{competitor_being_analyzed} ra mắt gói vay margin Super X với lãi suất hấp dẫn.",
        "analysis_summary_llm_ai": "Bài viết tập trung giới thiệu lợi ích và cách thức tham gia chương trình vay margin mới của {competitor_being_analyzed} nhằm thu hút nhà đầu tư.",
        "article_focus_ai": "Sản phẩm/Dịch vụ",
        "detailed_classification_ai": "PR Sản phẩm mới tiềm năng",
        "tone_ai": "Quảng bá/Ca ngợi"
    }}
    """
    safety_settings = [ # Giữ nguyên safety settings
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    ]
    generation_config = genai.types.GenerationConfig(
        temperature=0.3, # Có thể tăng nhẹ để AI linh hoạt hơn trong phân loại
        response_mime_type="application/json" # Yêu cầu Gemini trả về JSON
    )

    try:
        response = model.generate_content(
            prompt,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        time.sleep(SLEEP_BETWEEN_AI_CALLS)

        ai_result_text = response.text
        # print(f"Raw AI response for {competitor_being_analyzed}: {ai_result_text[:300]}") # Debug

        try:
            ai_data = json.loads(ai_result_text)
        except json.JSONDecodeError:
            # Thử trích xuất JSON từ markdown nếu có
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', ai_result_text, re.DOTALL)
            if json_match:
                cleaned_text = json_match.group(1)
                ai_data = json.loads(cleaned_text)
            else: # Nếu không có markdown, thử tìm JSON trực tiếp
                json_match_direct = re.search(r'(\{.*?\})', ai_result_text, re.DOTALL)
                if json_match_direct:
                    ai_data = json.loads(json_match_direct.group(1))
                else:
                    print(f"  [AI Analysis] Lỗi: Gemini response không phải JSON hợp lệ và không trích xuất được. Response: {ai_result_text[:300]}")
                    return {**default_result, "analysis_summary_llm_ai": f"Lỗi giải mã JSON: {ai_result_text[:200]}"}

        # Kiểm tra các khóa bắt buộc
        required_keys = default_result.keys() - {"_error"} # Trừ _error ra khỏi các key cần check
        if not all(key in ai_data for key in required_keys):
            missing_keys = [key for key in required_keys if key not in ai_data]
            print(f"  [AI Analysis] Lỗi: Kết quả AI thiếu các khóa bắt buộc: {missing_keys}. Parsed: {ai_data}")
            # Cố gắng điền các giá trị mặc định cho các khóa bị thiếu
            for key in missing_keys:
                ai_data[key] = default_result[key] # Gán giá trị mặc định
            # return {**default_result, "analysis_summary_llm_ai": f"Lỗi thiếu khóa JSON: {str(ai_data)[:200]}"}

        ai_data["_error"] = False
        print(f"  [AI Analysis] Phân tích cho '{competitor_being_analyzed}' thành công. Loại QC: {ai_data.get('is_advertising')}")
        return ai_data

    except Exception as e:
        print(f"  [AI Analysis] Lỗi khi gọi Gemini cho '{competitor_being_analyzed}': {e}")
        # traceback.print_exc(limit=1)
        error_summary = f"Lỗi Exception AI: {str(e)[:100]}"
        if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback') and e.response.prompt_feedback:
            error_summary += f" (Feedback: {e.response.prompt_feedback})"
        return {**default_result, "analysis_summary_llm_ai": error_summary}


# --- Time Parsing, Fetch Article Details, Find Keywords (Giữ nguyên) ---
def parse_article_time(date_text: str, current_time_utc: datetime) -> datetime | None:
    # ... (Code parse_article_time giữ nguyên) ...
    if not date_text: return None
    now_utc = current_time_utc
    local_tz = pytz.timezone(TARGET_TIMEZONE)
    now_local = now_utc.astimezone(local_tz)
    article_time_naive = None; parsed_format = None
    match = re.fullmatch(r'(\d{1,2})/(\d{1,2})\s+(\d{1,2}):(\d{1,2})', date_text)
    if match:
        try:
            day, month, hour, minute = map(int, match.groups()); year = now_local.year
            article_time_naive = datetime(year, month, day, hour, minute)
            if local_tz.localize(article_time_naive) > now_local + timedelta(hours=1): article_time_naive = article_time_naive.replace(year=year - 1)
            parsed_format = "DD/MM HH:MM"
        except ValueError: pass
    if not parsed_format:
        match = re.fullmatch(r'(\d{1,2})/(\d{1,2})/(\d{4})\s+(\d{1,2}):(\d{1,2})', date_text)
        if match:
            try:
                day, month, year, hour, minute = map(int, match.groups())
                if 1990 < year < 2100: article_time_naive = datetime(year, month, day, hour, minute); parsed_format = "DD/MM/YYYY HH:MM"
            except ValueError: pass
    if article_time_naive and parsed_format:
        try: return local_tz.localize(article_time_naive, is_dst=None).astimezone(pytz.utc)
        except Exception: return None
    for rel_pattern, unit, delta_func in [
        (r'(\d+)\s*(phút|phut|p|minute|min)s?\s*(trước|truoc|ago)', 'minutes', timedelta),
        (r'(\d+)\s*(giờ|gio|g|h|hour)s?\s*(trước|truoc|ago)', 'hours', timedelta),
        (r'(\d+)\s*(ngày|ngay|d|day)s?\s*(trước|truoc|ago)', 'days', timedelta)
    ]:
        match = re.fullmatch(rel_pattern, date_text, re.IGNORECASE)
        if match:
            try: return now_utc - delta_func(**{unit: int(match.group(1))})
            except ValueError: pass
    try:
        article_time_aware = datetime.fromisoformat(date_text.replace(" ", "T"))
        if article_time_aware.tzinfo is None or article_time_aware.tzinfo.utcoffset(article_time_aware) is None:
            article_time_aware = local_tz.localize(article_time_aware)
        return article_time_aware.astimezone(pytz.utc)
    except Exception: pass
    return None

def fetch_article_details(article_url: str, start_crawl_time_utc: datetime) -> tuple[str, str | None, datetime | None]:
    # ... (Code fetch_article_details giữ nguyên, đảm bảo trả về full content) ...
    content = "Lỗi tải nội dung"; date_text_detail = None; article_time_utc = None
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
        response = requests.get(article_url, headers=headers, timeout=REQUESTS_TIMEOUT, allow_redirects=True)
        response.raise_for_status()
        try: response.encoding = response.apparent_encoding; html_content = response.text
        except Exception: response.encoding = 'utf-8'; html_content = response.text
        detail_soup = BeautifulSoup(html_content, 'html.parser')
        time_selectors = ["meta[property='article:published_time']", "meta[itemprop='datePublished']", ".date", ".time", ".article__meta > time", "span.source-date", "time[datetime]"]
        for selector in time_selectors:
            el = detail_soup.select_one(selector)
            if el:
                dt_text = el.get('content') or el.get('datetime') or el.get_text(strip=True)
                if dt_text: date_text_detail = dt_text.strip(); article_time_utc = parse_article_time(date_text_detail, start_crawl_time_utc); break
        content_selectors = ["div[itemprop='articleBody']", "#vst_detail", ".content-news", ".article__body", ".detail-content", "article.content", "div.article-content"]
        content_area = next((detail_soup.select_one(s) for s in content_selectors if detail_soup.select_one(s)), None)
        if content_area:
            for el_to_remove in content_area.select('script, style, iframe, .adsbygoogle, .related-posts, figure.op-interactive, aside'): el_to_remove.decompose()
            # Lấy toàn bộ text, bao gồm cả heading, list items,...
            # content = content_area.get_text(separator="\n", strip=True)
            # Hoặc chi tiết hơn:
            text_parts = []
            for element in content_area.find_all(True, recursive=True): # Lấy tất cả các thẻ con
                if element.name in ['script', 'style', 'noscript', 'iframe', 'a', 'button']: continue # Bỏ qua các thẻ không mong muốn
                text = element.string # Lấy text trực tiếp của thẻ (không bao gồm con)
                if text and text.strip():
                    text_parts.append(text.strip())
            content = "\n".join(text_parts)
            content = re.sub(r'\n\s*\n', '\n', content).strip()
            if not content or len(content) < 100 : # Fallback nếu cách trên không hiệu quả
                 content = content_area.get_text(separator="\n", strip=True)
                 content = re.sub(r'\n\s*\n', '\n', content).strip()

        else: content = "Không tìm thấy nội dung"
    except requests.exceptions.HTTPError as http_err: content = f"Lỗi tải nội dung ({http_err.response.status_code})"
    except requests.exceptions.RequestException as req_err: content = f"Lỗi tải nội dung ({type(req_err).__name__})"
    except Exception as detail_err: content = f"Lỗi xử lý nội dung ({type(detail_err).__name__})"
    return content, date_text_detail, article_time_utc


def find_keywords_and_competitors(text_lower: str, title_lower: str) -> tuple[set, list]:
    # ... (Code find_keywords_and_competitors giữ nguyên) ...
    matched_competitors_keys = set() # Set các key của đối thủ (VPS, SSI)
    matched_keywords_list = [] # List các từ khóa cụ thể tìm thấy
    combined_text = title_lower + "\n" + text_lower
    for comp_key, kws_list in KEYWORDS.items():
        for kw in kws_list:
            pattern = r'\b' + re.escape(kw.lower()) + r'\b'
            if re.search(pattern, combined_text):
                matched_competitors_keys.add(comp_key)
                if kw not in matched_keywords_list: # Chỉ thêm keyword cụ thể một lần
                     matched_keywords_list.append(kw)
    return matched_competitors_keys, sorted(list(set(matched_keywords_list))) # Đảm bảo unique và sorted

# --- Fuzzy Match Program Name with Database ---
def match_program_with_db(
    program_name_from_ai: str,
    competitor_key: str, # e.g. "VPS"
    program_databases: dict, # Dict chứa DB của tất cả các công ty
    threshold: int = PROGRAM_SIMILARITY_THRESHOLD
) -> tuple[str | None, str]: # (matched_db_program_name, status_message)
    """
    Matches AI-extracted program name against a specific competitor's program database.
    """
    if not program_name_from_ai or not competitor_key:
        return None, "Thiếu tên chương trình từ AI hoặc tên công ty."

    competitor_db = program_databases.get(competitor_key)
    if not competitor_db:
        return program_name_from_ai, f"Sản phẩm mới tiềm năng (Không có DB cho {competitor_key})"

    program_names_in_db = [p["name"] for p in competitor_db]
    if not program_names_in_db:
        return program_name_from_ai, f"Sản phẩm mới tiềm năng (DB của {competitor_key} rỗng)"

    # fuzzywuzzy.process.extractOne returns (best_match, score)
    best_match = fuzzy_process.extractOne(program_name_from_ai, program_names_in_db, scorer=fuzz.token_set_ratio)

    if best_match and best_match[1] >= threshold:
        # print(f"    Fuzzy match for '{program_name_from_ai}' ({competitor_key}): Found '{best_match[0]}' with score {best_match[1]}%")
        return best_match[0], f"Khớp với DB ({best_match[1]}%)" # Trả về tên chương trình trong DB
    else:
        # print(f"    Fuzzy match for '{program_name_from_ai}' ({competitor_key}): No good match found (Best: {best_match[0]} at {best_match[1]}%). Potential new program.")
        return program_name_from_ai, "Sản phẩm mới tiềm năng (Cần review)"

print("Helper functions defined.")

# Cell 4: Core Processing Logic

def process_single_article_for_competitor(
    article_title: str,
    article_url: str,
    full_content: str, # Nội dung đầy đủ của bài viết
    competitor_key_being_analyzed: str, # Key của công ty đang được phân tích (e.g., "VPS")
    all_keywords_found_in_article: list, # List tất cả keyword tìm thấy trong bài
    crawl_time_str_local: str,
    gemini_model: genai.GenerativeModel,
    all_program_dbs: dict # Database chương trình của tất cả các công ty
) -> dict | None:
    """
    Processes a single article focusing on one competitor.
    Performs AI analysis and then matches program name with DB.
    Returns a dictionary ready for the output sheet.
    """
    print(f"-> Processing article '{article_title[:60]}' for competitor '{competitor_key_being_analyzed}'")

    # --- AI Analysis (Call 1) ---
    ai_analysis_result = analyze_article_with_gemini_v2(
        gemini_model,
        article_title,
        full_content, # Truyền full content, AI function sẽ tự truncate
        competitor_key_being_analyzed,
        all_keywords_found_in_article
    )

    # --- Prepare data for the output sheet ---
    output_row = {header: None for header in SHEET_HEADER_OUTPUT} # Khởi tạo với None

    output_row['Thời gian cập nhật'] = crawl_time_str_local
    output_row['Tiêu đề bài viết'] = article_title
    output_row['Công ty được phân tích'] = competitor_key_being_analyzed
    # 'Là công ty đối thủ?' sẽ được xác định trong hàm crawl_vietstock dựa trên KEYWORDS
    output_row['URL'] = article_url
    output_row['Nội dung gốc'] = full_content[:MAX_CONTENT_LENGTH_FOR_AI] # Lưu trích đoạn hoặc full nếu muốn

    if ai_analysis_result.get("_error"):
        output_row['Là quảng cáo? (AI)'] = "Lỗi AI"
        output_row['Thông điệp chính (AI)'] = ai_analysis_result.get("main_message_ai", "Lỗi AI")
        output_row['Tóm tắt phân tích LLM (AI)'] = ai_analysis_result.get("analysis_summary_llm_ai", "Lỗi AI")
        output_row['Trạng thái xử lý'] = "Lỗi phân tích AI"
        output_row['Trọng tâm bài viết (AI)'] = "Lỗi AI"
        output_row['Phân loại chi tiết (AI/DB)'] = "Lỗi AI"
        output_row['Giọng điệu (AI)'] = "Lỗi AI"
        return output_row

    # Điền các trường từ kết quả AI
    output_row['Là quảng cáo? (AI)'] = ai_analysis_result.get("is_advertising")
    output_row['Thông điệp chính (AI)'] = ai_analysis_result.get("main_message_ai")
    output_row['Tóm tắt phân tích LLM (AI)'] = ai_analysis_result.get("analysis_summary_llm_ai")
    output_row['Trọng tâm bài viết (AI)'] = ai_analysis_result.get("article_focus_ai")
    output_row['Phân loại chi tiết (AI/DB)'] = ai_analysis_result.get("detailed_classification_ai") # Sẽ cập nhật sau nếu là SP
    output_row['Giọng điệu (AI)'] = ai_analysis_result.get("tone_ai")
    output_row['Trạng thái xử lý'] = "Đã xử lý AI" # Trạng thái ban đầu

    program_name_ai = ai_analysis_result.get("promoted_program_product_name_ai")
    is_program_pr_ai = output_row['Là quảng cáo? (AI)'] == "PR Chương trình/Sản phẩm"

    final_program_name_display = None
    program_status_message = ""

    if is_program_pr_ai and program_name_ai:
        matched_db_name, status_msg = match_program_with_db(
            program_name_ai,
            competitor_key_being_analyzed,
            all_program_dbs # Truyền DB của tất cả công ty
        )
        program_status_message = status_msg

        if "Khớp với DB" in status_msg:
            final_program_name_display = matched_db_name # Hiển thị tên từ DB nếu khớp
            output_row['Phân loại chi tiết (AI/DB)'] = f"PR Sản phẩm hiện có ({competitor_key_being_analyzed})"
            output_row['Trạng thái xử lý'] = "Đã xử lý - SP khớp DB"
        elif "Sản phẩm mới tiềm năng" in status_msg:
            final_program_name_display = program_name_ai # Hiển thị tên từ AI
            output_row['Phân loại chi tiết (AI/DB)'] = f"PR Sản phẩm mới tiềm năng ({competitor_key_being_analyzed})"
            output_row['Trạng thái xử lý'] = "Đã xử lý - SP mới tiềm năng"
        else: # Trường hợp khác
            final_program_name_display = program_name_ai
            output_row['Trạng thái xử lý'] = f"Đã xử lý - {status_msg}"

        output_row['Tên chương trình/sản phẩm được nhắc đến'] = final_program_name_display
        print(f"    Program for '{competitor_key_being_analyzed}': '{final_program_name_display}'. Status: {program_status_message}")

    elif is_program_pr_ai and not program_name_ai:
        output_row['Tên chương trình/sản phẩm được nhắc đến'] = "AI xác định PR SP nhưng không trích xuất được tên"
        output_row['Trạng thái xử lý'] = "Lỗi AI trích xuất tên SP"
    # Các trường hợp khác (PR Thương hiệu, Không QC) thì 'Tên chương trình/sản phẩm được nhắc đến' sẽ là None

    # Nếu là PR Thương hiệu, cập nhật Phân loại chi tiết
    if output_row['Là quảng cáo? (AI)'] == "PR Thương hiệu":
        if "Lãnh đạo công ty" in str(output_row['Trọng tâm bài viết (AI)']):
            output_row['Phân loại chi tiết (AI/DB)'] = f"PR Thương hiệu - Bài viết về lãnh đạo ({competitor_key_being_analyzed})"
        else:
            output_row['Phân loại chi tiết (AI/DB)'] = f"PR Thương hiệu ({competitor_key_being_analyzed})"
        output_row['Trạng thái xử lý'] = "Đã xử lý - PR Thương hiệu"


    if output_row['Là quảng cáo? (AI)'] == "Không Quảng Cáo":
        output_row['Phân loại chi tiết (AI/DB)'] = f"Không quảng cáo ({competitor_key_being_analyzed})"
        output_row['Trạng thái xử lý'] = "Đã xử lý - Không quảng cáo"

    return output_row


# --- Write to Google Sheet (Chung, dùng lại hàm write_data_to_sheet từ gợi ý trước) ---
def write_data_to_google_sheet(
    gc: gspread.Client,
    sheet_id: str,
    worksheet_name: str,
    header: list,
    data_rows_as_dicts: list
) -> bool:
    if not data_rows_as_dicts:
        print(f"No data to write to Google Sheet '{worksheet_name}'.")
        return True # Coi như thành công vì không có gì để ghi
    if not gc:
        print(f"Error: Google Sheets client not available. Cannot write to '{worksheet_name}'.")
        return False

    print(f"\nPreparing {len(data_rows_as_dicts)} rows for Google Sheet '{worksheet_name}'...")
    rows_to_add_final = []
    for item_dict in data_rows_as_dicts:
        row_as_list = [item_dict.get(col_name, '') for col_name in header] # Dùng '' cho giá trị rỗng
        rows_to_add_final.append(row_as_list)

    if not rows_to_add_final:
        print(f"No final rows generated to write to '{worksheet_name}'.")
        return True

    print(f"Attempting to write {len(rows_to_add_final)} rows to '{worksheet_name}'...")
    try:
        spreadsheet = gc.open_by_key(sheet_id)
        try:
            worksheet = spreadsheet.worksheet(worksheet_name)
            current_header_gs = []
            try:
                current_header_gs = worksheet.row_values(1)
            except Exception: # Có thể sheet mới hoàn toàn, chưa có dòng nào
                pass

            if not current_header_gs or current_header_gs != header:
                print(f"   Updating header for sheet '{worksheet_name}'...")
                # Xóa dữ liệu cũ nếu header thay đổi (tùy chọn, cẩn thận)
                # worksheet.clear()
                worksheet.update(range_name='A1', values=[header], value_input_option='USER_ENTERED')
                worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(header))}', {'textFormat': {'bold': True}})
                print(f"   Header updated/set for '{worksheet_name}'.")

        except gspread.exceptions.WorksheetNotFound:
            print(f"   Worksheet '{worksheet_name}' not found. Creating...")
            worksheet = spreadsheet.add_worksheet(title=worksheet_name, rows="100", cols=len(header) + 5) # Thêm cột dự phòng
            worksheet.update(range_name='A1', values=[header], value_input_option='USER_ENTERED')
            worksheet.format(f'A1:{gspread.utils.rowcol_to_a1(1, len(header))}', {'textFormat': {'bold': True}})
            print(f"   Worksheet '{worksheet_name}' created with header.")

        # Append data in batches
        num_rows_to_write = len(rows_to_add_final)
        for i in range(0, num_rows_to_write, SHEET_WRITE_BATCH_SIZE):
            batch = rows_to_add_final[i:min(i + SHEET_WRITE_BATCH_SIZE, num_rows_to_write)]
            print(f"   Writing batch {i // SHEET_WRITE_BATCH_SIZE + 1} ({len(batch)} rows) to '{worksheet_name}'...")
            worksheet.append_rows(batch, value_input_option='USER_ENTERED', table_range='A1')
            if num_rows_to_write > SHEET_WRITE_BATCH_SIZE and (i + SHEET_WRITE_BATCH_SIZE < num_rows_to_write):
                time.sleep(SHEET_WRITE_DELAY)

        print(f"✅ Successfully wrote {len(rows_to_add_final)} rows to '{worksheet_name}'.")
        return True
    except gspread.exceptions.APIError as api_err:
        print(f"LỖI API Google Sheets khi ghi vào '{worksheet_name}': {api_err}")
        if 'PERMISSION_DENIED' in str(api_err):
             print("   Hint: Ensure the Service Account has 'Editor' permissions on this Google Sheet.")
        return False
    except Exception as sheet_err:
        print(f"LỖI không xác định khi ghi vào Google Sheet '{worksheet_name}': {sheet_err}")
        traceback.print_exc()
        return False

# --- Main Crawling Function ---
def crawl_vietstock():
    start_crawl_time_local = get_vietnam_time()
    start_crawl_time_utc = start_crawl_time_local.astimezone(pytz.utc)
    crawl_time_str_local = start_crawl_time_local.strftime('%Y-%m-%d %H:%M:%S')
    print(f"[{crawl_time_str_local} {start_crawl_time_local.tzname()}] Bắt đầu crawl Vietstock (Kiểm tra {HOURS_TO_CHECK} giờ gần nhất)...")

    time_limit_utc = start_crawl_time_utc - timedelta(hours=HOURS_TO_CHECK)

    # --- Initializations ---
    gc = None
    driver = None
    gemini_model = None
    all_program_databases = {} # { "VPS": [{"name":..., "details":...}], ... }
    all_output_data_for_sheet = [] # List of dicts, mỗi dict là 1 dòng cho sheet output
    all_articles_scraped_for_xlsx_log = [] # Log tất cả bài viết đã gặp
    processed_article_urls_for_content = set() # URLs đã lấy nội dung chi tiết

    try:
        gc = authenticate_google_sheets_service_account(SERVICE_ACCOUNT_JSON_PATH)
        if not gc:
            print("CRITICAL: Không thể xác thực Google Sheets. Dừng chương trình.")
            return

        try:
            driver = setup_selenium_driver()
        except Exception as driver_err:
            print(f"CRITICAL: Không thể khởi tạo Selenium WebDriver: {driver_err}. Dừng chương trình.")
            return

        gemini_model = setup_gemini(GEMINI_API_KEY)
        if not gemini_model:
            print("WARNING: Không thể khởi tạo Gemini Model. Phân tích AI sẽ bị bỏ qua hoặc lỗi.")
            # Không dừng hẳn, nhưng các trường AI sẽ báo lỗi

        all_program_databases = read_all_competitor_program_dbs(gc, GOOGLE_SHEET_ID, COMPETITOR_PROGRAM_DB_SHEET_MAP)

        # --- Crawling Loop ---
        print(f"\nTruy cập trang bắt đầu: {VIETSTOCK_START_URL}")
        driver.get(VIETSTOCK_START_URL)
        current_page = 1
        stop_crawling = False

        while current_page <= MAX_PAGES_TO_CRAWL and not stop_crawling:
            print(f"\n--- Processing Page {current_page}/{MAX_PAGES_TO_CRAWL} ---")
            page_start_time = time.time()
            old_article_found_on_page = False

            try:
                wait = WebDriverWait(driver, SELENIUM_WAIT)
                # Chờ container chính của danh sách bài viết
                container_selectors = ["#channel-container", "#category_content", "div.listing-news", "ul.list-news-font-size-medium"]
                container = None
                for sel in container_selectors:
                    try:
                        container = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, sel)))
                        if container: break
                    except TimeoutException:
                        continue
                if not container:
                    print(f"   Warning: Timed out waiting for any article container on page {current_page}.")
                    if current_page == 1 and ("<body" not in driver.page_source or len(driver.page_source) < 1000):
                        print("   Page 1 seems empty or failed to load. Stopping.")
                        break # Dừng nếu trang đầu lỗi nặng
                    # Có thể thử refresh hoặc bỏ qua trang này
                    current_page +=1 # Thử sang trang tiếp
                    time.sleep(SLEEP_BETWEEN_PAGES)
                    try: # Thử click next page nếu có
                        next_button_sel = driver.find_element(By.CSS_SELECTOR, '.btn-page-next, a[title="Trang sau"], a.next')
                        if next_button_sel.is_displayed() and next_button_sel.is_enabled():
                            driver.execute_script("arguments[0].click();", next_button_sel)
                            time.sleep(SLEEP_BETWEEN_PAGES)
                            continue
                        else: break
                    except: break


                time.sleep(2) # Chờ thêm chút cho JS load (nếu có)
            except Exception as wait_err:
                print(f"   Error waiting for page elements: {wait_err}")
                break # Dừng nếu không tìm thấy container

            soup = BeautifulSoup(driver.page_source, 'html.parser')
            # Điều chỉnh selector cho các bài viết dựa trên cấu trúc thực tế của Vietstock
            # Ví dụ: các thẻ div hoặc li chứa thông tin bài viết
            article_elements_soup = soup.select("div.single_post_text")
            if not article_elements_soup: # Fallback selectors
                article_elements_soup = soup.select("div.mb-3 > h3 > a") # Tìm các thẻ a là tiêu đề
                article_elements_soup = [a.find_parent('div', class_=re.compile(r'item|article')) or a.find_parent('li') or a for a in article_elements_soup]


            print(f"   Found {len(article_elements_soup)} potential articles on page {current_page}.")
            if not article_elements_soup and current_page == 1:
                print("   No articles found on the first page. Check selectors or website structure.")
                break

            for article_soup_item in article_elements_soup: # article_soup_item giờ là div.single_post_text (hoặc khối cha nếu dùng Cách 2)
                title = "N/A"; article_url = None; article_time_list_utc = None
                try:
                    # Lấy link và title
                    # Nếu article_soup_item là div.single_post_text:
                    title_link_element = article_soup_item.select_one("h4 > a.fontbold")
                    if not title_link_element:
                         title_link_element = article_soup_item.select_one("h4 > a") # Fallback

                    # Nếu article_soup_item là khối cha (ví dụ div.article-wrapper), thì phải select sâu hơn:
                    # title_link_element = article_soup_item.select_one("div.single_post_text > h4 > a.fontbold")

                    if not title_link_element:
                        # print("    Skipping item, title_link_element not found.") # Debug
                        continue

                    title = title_link_element.get_text(strip=True) or title_link_element.get('title', 'N/A').strip()
                    raw_url = title_link_element.get('href')
                    if not raw_url:
                        # print(f"    Skipping item '{title[:30]}...', raw_url not found.") # Debug
                        continue

                    # ... (xử lý raw_url thành article_url như cũ) ...
                    if raw_url.startswith('http'): article_url = raw_url
                    elif raw_url.startswith('/'): article_url = BASE_URL + raw_url # Giả sử BASE_URL là "https://vietstock.vn" hoặc "http://fili.vn"
                    else: article_url = BASE_URL + '/' + raw_url.lstrip('/')
                    article_url = article_url.strip().split("?")[0]

                    # Lấy thời gian từ trang list
                    # Nếu article_soup_item là div.single_post_text:
                    time_element = article_soup_item.select_one("div.meta3 > a:nth-of-type(2)")
                    # Nếu article_soup_item là khối cha:
                    # time_element = article_soup_item.select_one("div.single_post_text > div.meta3 > a:nth-of-type(2)")

                    date_text_list = None
                    if time_element:
                        # Thử lấy text trực tiếp, vì ::before có thể không được get_text() lấy
                        # Cần kiểm tra xem text của thẻ <a> này có chứa ngày giờ không, hay nó nằm ở đâu đó khác
                        # Nếu text nằm trong ::before, BeautifulSoup không lấy được trực tiếp.
                        # Bạn có thể cần dùng Selenium để lấy computed style hoặc text content bao gồm pseudo-elements,
                        # hoặc tìm một nguồn thời gian khác trên trang.
                        # Tạm thời, thử get_text() và xem nó trả về gì.
                        date_text_list = time_element.get_text(strip=True)
                        # print(f"    Raw time text from list: '{date_text_list}' for article '{title[:30]}'") # Debug
                        if date_text_list: # Cần xử lý chuỗi này nếu nó không phải định dạng chuẩn
                            # Ví dụ, nếu nó là "QUY HOẠCH - HẠ TẦNG | 28/05 16:56"
                            # Cần trích xuất "28/05 16:56"
                            match_time = re.search(r'(\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2})', date_text_list)
                            if match_time:
                                date_text_list = match_time.group(1)
                                # print(f"    Extracted time text: '{date_text_list}'") # Debug
                                article_time_list_utc = parse_article_time(date_text_list, start_crawl_time_utc)
                            else: # Nếu không trích xuất được, thử tìm trong title của thẻ a
                                time_title_attr = time_element.get('title')
                                if time_title_attr:
                                     match_time_title = re.search(r'(\d{1,2}/\d{1,2}/\d{4}\s+\d{1,2}:\d{1,2}:\d{1,2})|(\d{1,2}/\d{1,2}\s+\d{1,2}:\d{1,2})', time_title_attr)
                                     if match_time_title:
                                         date_text_list = match_time_title.group(0) # Lấy toàn bộ match
                                         article_time_list_utc = parse_article_time(date_text_list, start_crawl_time_utc)

                    # Kiểm tra nếu bài quá cũ (dựa trên thời gian từ trang list)
                    if article_time_list_utc and article_time_list_utc < time_limit_utc:
                        print(f"      Article '{title[:40]}...' (list time) too old. Stopping page processing.")
                        old_article_found_on_page = True; break # Dừng xử lý các bài còn lại trên trang này

                    # Log tất cả bài viết đã gặp (cho file XLSX)
                    temp_article_time_str = article_time_list_utc.astimezone(pytz.timezone(TARGET_TIMEZONE)).strftime('%Y-%m-%d %H:%M:%S') if article_time_list_utc else "N/A"
                    all_articles_scraped_for_xlsx_log.append({
                        'URL': article_url, 'Tiêu đề': title,
                        'Thời gian (List View)': temp_article_time_str, 'Nguồn': 'Vietstock'
                    })

                    # --- Xử lý sâu nếu URL chưa được lấy nội dung ---
                    if article_url not in processed_article_urls_for_content:
                        processed_article_urls_for_content.add(article_url) # Đánh dấu đã xử lý URL này

                        print(f"\n  Fetching details for: {title[:60]}... ({article_url})")
                        full_content, _, article_time_detail_utc = fetch_article_details(article_url, start_crawl_time_utc)

                        # Ưu tiên thời gian từ trang chi tiết, nếu không có thì dùng thời gian từ trang list
                        final_article_time_utc = article_time_detail_utc or article_time_list_utc
                        if final_article_time_utc and final_article_time_utc < time_limit_utc:
                            print(f"    Article '{title[:40]}...' (detail/final time) too old. Skipping further analysis.")
                            continue # Bỏ qua bài này, nhưng không dừng cả trang (có thể bài sau mới hơn)

                        if isinstance(full_content, str) and "Lỗi" not in full_content and len(full_content) > 100:
                            # Tìm keywords trong title và content
                            title_lower = title.lower()
                            content_lower = full_content.lower() # Dùng full_content đã lấy
                            matched_competitor_keys, all_kws_found = find_keywords_and_competitors(content_lower, title_lower)

                            if matched_competitor_keys:
                                print(f"    Keywords found for: {', '.join(sorted(list(matched_competitor_keys)))} in '{title[:50]}...'")
                                # Với mỗi công ty đối thủ tìm thấy trong bài, chạy phân tích
                                for comp_key in matched_competitor_keys:
                                    if not gemini_model:
                                        print(f"    Skipping AI for {comp_key} due to Gemini model init failure.")
                                        # Tạo một bản ghi lỗi cơ bản
                                        error_output_row = {h: None for h in SHEET_HEADER_OUTPUT}
                                        error_output_row.update({
                                            'Thời gian cập nhật': crawl_time_str_local, 'Tiêu đề bài viết': title,
                                            'Công ty được phân tích': comp_key, 'URL': article_url,
                                            'Là công ty đối thủ?': True, 'Trạng thái xử lý': "Lỗi Model AI",
                                            'Nội dung gốc': full_content[:MAX_CONTENT_LENGTH_FOR_AI]
                                        })
                                        all_output_data_for_sheet.append(error_output_row)
                                        continue

                                    # Gọi hàm xử lý chính cho từng đối thủ
                                    output_data_row = process_single_article_for_competitor(
                                        title, article_url, full_content,
                                        comp_key, all_kws_found, # Truyền list keyword tìm thấy
                                        crawl_time_str_local,
                                        gemini_model, all_program_databases
                                    )
                                    if output_data_row:
                                        output_data_row['Là công ty đối thủ?'] = True # Vì comp_key nằm trong matched_competitor_keys
                                        all_output_data_for_sheet.append(output_data_row)
                            else:
                                print(f"    No relevant keywords in '{title[:50]}...'. Skipping AI analysis for this article.")
                                # Vẫn có thể ghi 1 dòng "Không có đối thủ" nếu muốn
                                no_competitor_row = {h: None for h in SHEET_HEADER_OUTPUT}
                                no_competitor_row.update({
                                    'Thời gian cập nhật': crawl_time_str_local, 'Tiêu đề bài viết': title,
                                    'Công ty được phân tích': "Không có", 'URL': article_url,
                                    'Là công ty đối thủ?': False, 'Trạng thái xử lý': "Không có từ khóa đối thủ",
                                    'Nội dung gốc': full_content[:MAX_CONTENT_LENGTH_FOR_AI]
                                })
                                all_output_data_for_sheet.append(no_competitor_row)
                        else:
                            print(f"    Content error or insufficient for '{title[:50]}...'. Skipping analysis.")
                            error_content_row = {h: None for h in SHEET_HEADER_OUTPUT}
                            error_content_row.update({
                                'Thời gian cập nhật': crawl_time_str_local, 'Tiêu đề bài viết': title,
                                'Công ty được phân tích': "N/A", 'URL': article_url,
                                'Là công ty đối thủ?': False, # Không biết
                                'Trạng thái xử lý': "Lỗi tải/xử lý nội dung",
                                'Nội dung gốc': str(full_content)[:200] # Ghi lại lỗi
                            })
                            all_output_data_for_sheet.append(error_content_row)
                    # else:
                    #     print(f"  Content for {article_url} already processed. Skipping.")

                except KeyboardInterrupt: raise
                except Exception as article_loop_err:
                    print(f"   LỖI khi xử lý một bài viết '{title[:50]}...': {article_loop_err}")
                    traceback.print_exc(limit=1)
                    # Ghi nhận lỗi này vào sheet
                    error_loop_row = {h: None for h in SHEET_HEADER_OUTPUT}
                    error_loop_row.update({
                        'Thời gian cập nhật': crawl_time_str_local, 'Tiêu đề bài viết': title or "Lỗi lấy tiêu đề",
                        'Công ty được phân tích': "N/A", 'URL': article_url or "Lỗi lấy URL",
                        'Là công ty đối thủ?': False,
                        'Trạng thái xử lý': f"Lỗi vòng lặp xử lý: {str(article_loop_err)[:100]}",
                        'Nội dung gốc': "Lỗi"
                    })
                    all_output_data_for_sheet.append(error_loop_row)


            if old_article_found_on_page:
                print("   Old article detected on page, stopping crawl for subsequent pages.")
                stop_crawling = True; break

            print(f"   Finished page {current_page}. Processing time: {time.time() - page_start_time:.2f}s.")
            if current_page >= MAX_PAGES_TO_CRAWL:
                print(f"   Reached max pages ({MAX_PAGES_TO_CRAWL}). Stopping."); break

            # --- Pagination ---
            current_page += 1
            print(f"   Attempting to navigate to page {current_page}...")
            try:
                # Thử các selectors phổ biến cho nút "Next" hoặc số trang tiếp theo
                next_button_selectors = [
                    f"//a[contains(@href, 'page={current_page}') and not(contains(@class, 'current'))]", # XPath cho link có page=
                    f"//a[normalize-space()='{current_page}']", # XPath cho số trang
                    "a.next", "a.next-page", "a[rel='next']",
                    "a[title*='Trang sau'], a[title*='Next']",
                    ".pagination li.active + li a", # Link trong li kế tiếp li active
                    "//a[contains(text(),'Sau') or contains(text(),'Tiếp') or contains(text(),'>')]"
                ]
                next_button = None
                for sel_type, sel_val in [("xpath", s) if s.startswith("//") else ("css",s) for s in next_button_selectors]:
                    try:
                        if sel_type == "xpath":
                            next_button = driver.find_element(By.XPATH, sel_val)
                        else: # css
                            next_button = driver.find_element(By.CSS_SELECTOR, sel_val)

                        if next_button and next_button.is_displayed() and next_button.is_enabled():
                            print(f"   Found next page element with selector: {sel_val}")
                            break
                        else: next_button = None
                    except NoSuchElementException:
                        next_button = None
                if next_button:
                    driver.execute_script("arguments[0].scrollIntoViewIfNeeded(true);", next_button)
                    time.sleep(0.5)
                    driver.execute_script("arguments[0].click();", next_button)
                    print(f"   Clicked 'Next' or page {current_page}. Waiting for page load...")
                    time.sleep(SLEEP_BETWEEN_PAGES)
                else:
                    print("   Could not find a 'Next' button or link for the next page. Stopping pagination.")
                    stop_crawling = True
            except Exception as page_err:
                print(f"   Error during pagination to page {current_page}: {page_err}")
                stop_crawling = True

    except KeyboardInterrupt:
        print("\nKeyboardInterrupt detected. Stopping crawl...")
    except Exception as e:
        print(f"\n--- UNEXPECTED ERROR DURING CRAWL ---")
        print(f"Error: {e}")
        traceback.print_exc()
    finally:
        if driver:
            print("Closing Selenium WebDriver...")
            driver.quit()

    # --- Save Results ---
    sheet_write_successful = False
    if gc and all_output_data_for_sheet:
        print(f"\nAttempting to write {len(all_output_data_for_sheet)} processed rows to Google Sheet '{TARGET_WORKSHEET_NAME}'...")
        sheet_write_successful = write_data_to_google_sheet(
            gc, GOOGLE_SHEET_ID, TARGET_WORKSHEET_NAME,
            SHEET_HEADER_OUTPUT, # Sử dụng header output đã định nghĩa
            all_output_data_for_sheet
        )
    elif not all_output_data_for_sheet:
        print("\nNo data processed to write to Google Sheet.")
    else:
        print("\nGoogle Sheets client not available or failed, cannot write to Sheet.")

    # --- Save XLSX Log (Thông tin cơ bản của tất cả bài đã gặp) ---
    if all_articles_scraped_for_xlsx_log:
        print(f"\nAttempting to save {len(all_articles_scraped_for_xlsx_log)} scraped article (log) details to XLSX...")
        xlsx_filename = f"ALL_vietstock_articles_LOG_{start_crawl_time_local.strftime('%Y%m%d_%H%M%S')}.xlsx"
        try:
            df_all_log = pd.DataFrame(all_articles_scraped_for_xlsx_log)
            df_all_log.to_excel(xlsx_filename, index=False, engine='openpyxl')
            print(f"✅ ***** ALL SCRAPED ARTICLE (LOG) INFO SAVED TO: {xlsx_filename} *****")
            try:
                from google.colab import files
                files.download(xlsx_filename)
            except Exception as download_err:
                print(f"   LỖI: Could not automatically download '{xlsx_filename}': {download_err}")
        except ImportError:
            print("WARNING: `openpyxl` not found. Cannot save XLSX log. Run `pip install openpyxl`")
        except Exception as xlsx_err:
            print(f"LỖI: Failed to save XLSX log file '{xlsx_filename}': {xlsx_err}")
    else:
        print("\nNo articles were logged to save to XLSX.")

    # --- Save CSV backup for the MAIN output sheet data IF Sheets failed ---
    if not sheet_write_successful and all_output_data_for_sheet:
        print(f"\nGoogle Sheet write failed or skipped. Saving main output data to local CSV backup...")
        try:
            df_backup = pd.DataFrame(all_output_data_for_sheet)
            # Đảm bảo các cột theo đúng thứ tự của SHEET_HEADER_OUTPUT
            df_backup = df_backup.reindex(columns=SHEET_HEADER_OUTPUT, fill_value='')
            backup_filename = f"vietstock_quangcao_BACKUP_{start_crawl_time_local.strftime('%Y%m%d_%H%M%S')}.csv"
            df_backup.to_csv(backup_filename, index=False, encoding='utf-8-sig')
            print(f"ℹ️ ***** Main output data SAVED TO LOCAL BACKUP CSV: {backup_filename} *****")
        except Exception as csv_err:
            print(f"LỖI: Failed to save backup CSV: {csv_err}")

    end_crawl_time_local = get_vietnam_time()
    print(f"\n[{end_crawl_time_local.strftime('%Y-%m-%d %H:%M:%S %Z')}] Vietstock crawl finished.")
    total_duration = end_crawl_time_local - start_crawl_time_local
    print(f"Total duration: {total_duration}")
    print(f"Total articles logged for XLSX: {len(all_articles_scraped_for_xlsx_log)}")
    print(f"Total rows prepared for Google Sheet '{TARGET_WORKSHEET_NAME}': {len(all_output_data_for_sheet)}")

print("Core processing logic defined.")

# Cell 5: Run Main Function
if __name__ == "__main__":
    # Make sure pandas is available for XLSX and CSV writing
    try:
        import pandas as pd
        import openpyxl # Needed for df.to_excel
    except ImportError as import_err:
        print(f"Import Error: {import_err}")
        print("Please ensure pandas and openpyxl are installed (`pip install pandas openpyxl`)")
        raise

    crawl_vietstock()